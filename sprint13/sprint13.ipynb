{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 584,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 585,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class GetMiniBatch:\n",
    "    \"\"\"\n",
    "    ミニバッチを取得するイテレータ\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : 次の形のndarray, shape (n_samples, n_features)\n",
    "      学習データ\n",
    "    y : 次の形のndarray, shape (n_samples, 1)\n",
    "      正解値\n",
    "    batch_size : int\n",
    "      バッチサイズ\n",
    "    seed : int\n",
    "      NumPyの乱数のシード\n",
    "    \"\"\"\n",
    "    def __init__(self, X, y, batch_size = 10, seed=0):\n",
    "        self.batch_size = batch_size\n",
    "        np.random.seed(seed)\n",
    "        shuffle_index = np.random.permutation(np.arange(X.shape[0]))\n",
    "        self.X = X[shuffle_index]\n",
    "        self.y = y[shuffle_index]\n",
    "        self._stop = np.ceil(X.shape[0]/self.batch_size).astype(np.int)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self._stop\n",
    "\n",
    "    def __getitem__(self,item):\n",
    "        p0 = item*self.batch_size\n",
    "        p1 = item*self.batch_size + self.batch_size\n",
    "        return self.X[p0:p1], self.y[p0:p1]        \n",
    "\n",
    "    def __iter__(self):\n",
    "        self._counter = 0\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        if self._counter >= self._stop:\n",
    "            raise StopIteration()\n",
    "        p0 = self._counter*self.batch_size\n",
    "        p1 = self._counter*self.batch_size + self.batch_size\n",
    "        self._counter += 1\n",
    "        return self.X[p0:p1], self.y[p0:p1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 586,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class FC:\n",
    "    \"\"\"\n",
    "    ノード数n_nodes1からn_nodes2への全結合層\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_nodes1 : int\n",
    "      前の層のノード数\n",
    "    n_nodes2 : int\n",
    "      後の層のノード数\n",
    "    initializer : 初期化方法のインスタンス\n",
    "    optimizer : 最適化手法のインスタンス\n",
    "    \"\"\"\n",
    "    def __init__(self, n_nodes1, n_nodes2, initializer, optimizer, dropout_rate=0.5):\n",
    "        self.optimizer = optimizer\n",
    "        # 初期化\n",
    "        # initializerのメソッドを使い、self.Wとself.Bを初期化する\n",
    "        self.W = initializer.W(n_nodes1, n_nodes2)\n",
    "        self.B = initializer.B(n_nodes2)\n",
    "        self.dZ = 0\n",
    "        self.dA = 0\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.mask = None\n",
    "        self.input_X_forward = 0\n",
    "        \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        フォワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (batch_size, n_nodes1)\n",
    "            入力\n",
    "        Returns\n",
    "        ----------\n",
    "        A : 次の形のndarray, shape (batch_size, n_nodes2)\n",
    "            出力\n",
    "        \"\"\" \n",
    "        self.input_X_forward = X\n",
    "        A = np.dot(X, self.W) + self.B\n",
    "        \n",
    "        return A\n",
    "    \n",
    "    def backward(self, dA):\n",
    "        \"\"\"\n",
    "        バックワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        dA : 次の形のndarray, shape (batch_size, n_nodes2)\n",
    "            後ろから流れてきた勾配\n",
    "        Returns\n",
    "        ----------\n",
    "        dZ : 次の形のndarray, shape (batch_size, n_nodes1)\n",
    "            前に流す勾配\n",
    "        \"\"\"\n",
    "        \n",
    "        dW = np.dot(self.input_X_forward.T, dA)\n",
    "        dZ = np.dot(dA, self.W.T)\n",
    "        self.dA = dA\n",
    "        self.dW = dW\n",
    "        self.dZ = dZ\n",
    "        # 更新\n",
    "        self = self.optimizer.update(self)\n",
    "        return dZ\n",
    "    \n",
    "    def dropout_forward(self, X, flag):\n",
    "        if flag:\n",
    "            self.mask = np.random.rand(*X.shape) > self.dropout_rate\n",
    "            return X * self.mask\n",
    "        else:\n",
    "            return X * (1.0 - self.dropout_rate)\n",
    "        \n",
    "    def dropout_backward(self, X): \n",
    "        return X * self.mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 633,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class FC2:\n",
    "    \"\"\"\n",
    "    ノード数n_nodes1からn_nodes2への全結合層\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_nodes1 : int\n",
    "      前の層のノード数\n",
    "    n_nodes2 : int\n",
    "      後の層のノード数\n",
    "    initializer : 初期化方法のインスタンス\n",
    "    optimizer : 最適化手法のインスタンス\n",
    "    \"\"\"\n",
    "    def __init__(self, n_nodes1, n_nodes2, initializer, optimizer, dropout_rate=0.5):\n",
    "        self.optimizer = optimizer\n",
    "        # 初期化\n",
    "        # initializerのメソッドを使い、self.Wとself.Bを初期化する\n",
    "        self.W = initializer.W(n_nodes1, n_nodes2)\n",
    "        self.B = initializer.B(n_nodes2)\n",
    "        self.W_feedback = 0\n",
    "        self.B_feedback = 0\n",
    "        self.dZ = 0\n",
    "        self.dA = 0\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.mask = None\n",
    "        self.input_X_forward = 0\n",
    "        \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        フォワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (batch_size, n_nodes1)\n",
    "            入力\n",
    "        Returns\n",
    "        ----------\n",
    "        A : 次の形のndarray, shape (batch_size, n_nodes2)\n",
    "            出力\n",
    "        \"\"\" \n",
    "        self.input_X_forward = X\n",
    "        A = np.dot(X, self.W) + self.B\n",
    "        \n",
    "        return A\n",
    "    \n",
    "    def backward(self, dA):\n",
    "        \"\"\"\n",
    "        バックワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        dA : 次の形のndarray, shape (batch_size, n_nodes2)\n",
    "            後ろから流れてきた勾配\n",
    "        Returns\n",
    "        ----------\n",
    "        dZ : 次の形のndarray, shape (batch_size, n_nodes1)\n",
    "            前に流す勾配\n",
    "        \"\"\"\n",
    "        \n",
    "        #print(\"input_X_forward.shape:\",self.input_X_forward.shape)\n",
    "        #print(\"dA.shape:\",dA.shape)\n",
    "        #print(\"self X_forward:\",self.input_X_forward.shape)\n",
    "        #print(\"dA:\",dA.shape)\n",
    "        dW = np.dot(self.input_X_forward.T, dA)\n",
    "        #print(\"dW:\",dW.shape)\n",
    "        dZ = np.dot(dA, self.W.T)\n",
    "        self.dA = dA\n",
    "        self.dW = dW\n",
    "        self.dZ = dZ\n",
    "        \n",
    "        self.W_feedback = self.dW / self.dA.shape[0]\n",
    "        self.B_feedback = np.average(self.dA, axis=0)\n",
    "        \n",
    "        # 更新\n",
    "        #print(\"W.shape:\",self.W.shape)\n",
    "        self = self.optimizer.update(self)\n",
    "        return dZ\n",
    "    \n",
    "    def dropout_forward(self, X, flag):\n",
    "        if flag:\n",
    "            self.mask = np.random.rand(*X.shape) > self.dropout_rate\n",
    "            return X * self.mask\n",
    "        else:\n",
    "            return X * (1.0 - self.dropout_rate)\n",
    "        \n",
    "    def dropout_backward(self, X): \n",
    "        return X * self.mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 587,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SimpleInitializer:\n",
    "    \"\"\"\n",
    "    ガウス分布によるシンプルな初期化\n",
    "    Parameters\n",
    "    ----------\n",
    "    sigma : float\n",
    "      ガウス分布の標準偏差\n",
    "    \"\"\"\n",
    "    def __init__(self, sigma = 0.01):\n",
    "        self.sigma = sigma\n",
    "        \n",
    "    def W(self, n_nodes1, n_nodes2):\n",
    "        W = self.sigma * np.random.randn(n_nodes1, n_nodes2)\n",
    "        return W\n",
    "    \n",
    "    def B(self, n_nodes2):\n",
    "        B = self.sigma * np.random.randn(1, n_nodes2)\n",
    "        return B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 588,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SGD:\n",
    "    \"\"\"\n",
    "    確率的勾配降下法\n",
    "    Parameters\n",
    "    ----------\n",
    "    lr : 学習率\n",
    "    \"\"\"\n",
    "    def __init__(self, lr):\n",
    "        self.lr = lr\n",
    "    def update(self, layer):\n",
    "        \"\"\"\n",
    "        ある層の重みやバイアスの更新\n",
    "        Parameters\n",
    "        ----------\n",
    "        layer : 更新前の層のインスタンス\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        layer : 更新後の層のインスタンス\n",
    "        \"\"\"\n",
    "        layer.B = layer.B - self.lr * layer.B_feedback    \n",
    "        layer.W = layer.W - self.lr * layer.W_feedback\n",
    "        \n",
    "        return layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 589,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Sigmoid:\n",
    "    \"\"\"\n",
    "    活性化関数 : Sigmoid\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_nodes1 : int\n",
    "      前の層のノード数\n",
    "    n_nodes2 : int\n",
    "      後の層のノード数\n",
    "    initializer : 初期化方法のインスタンス\n",
    "    optimizer : 最適化手法のインスタンス\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        # 初期化\n",
    "        self.input_X_forward = 0\n",
    "    \n",
    "    def _func(self, X):\n",
    "        return 1 / (1 + np.exp(-1 * X))\n",
    "    \n",
    "    def _func_diff(self, X):\n",
    "        return (1 - self._func(X)) * self._func(X)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        フォワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (batch_size, n_nodes1)\n",
    "            入力\n",
    "        Returns\n",
    "        ----------\n",
    "        A : 次の形のndarray, shape (batch_size, n_nodes2)\n",
    "            出力\n",
    "        \"\"\" \n",
    "        self.input_X_forward = X\n",
    "        A = self._func(X)\n",
    "        return A\n",
    "    \n",
    "    def backward(self, dA):\n",
    "        \"\"\"\n",
    "        バックワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        dA : 次の形のndarray, shape (batch_size, n_nodes2)\n",
    "            後ろから流れてきた勾配\n",
    "        Returns\n",
    "        ----------\n",
    "        dZ : 次の形のndarray, shape (batch_size, n_nodes1)\n",
    "            前に流す勾配\n",
    "        \"\"\"\n",
    "        \n",
    "        grad = self._func_diff(self.input_X_forward)\n",
    "        dZ = grad * dA\n",
    "        return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 590,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Tanh:\n",
    "    \"\"\"\n",
    "    活性化関数 : Sigmoid\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_nodes1 : int\n",
    "      前の層のノード数\n",
    "    n_nodes2 : int\n",
    "      後の層のノード数\n",
    "    initializer : 初期化方法のインスタンス\n",
    "    optimizer : 最適化手法のインスタンス\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        # 初期化\n",
    "        self.input_X_forward = 0\n",
    "    \n",
    "    def _func(self, X):\n",
    "        return np.tanh(X)\n",
    "    \n",
    "    def _func_diff(self, X):\n",
    "        return 1 - (self._func(X))**2\n",
    "        \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        フォワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (batch_size, n_nodes1)\n",
    "            入力\n",
    "        Returns\n",
    "        ----------\n",
    "        A : 次の形のndarray, shape (batch_size, n_nodes2)\n",
    "            出力\n",
    "        \"\"\" \n",
    "        self.input_X_forward = X\n",
    "        A = self._func(X)\n",
    "        return A\n",
    "    \n",
    "    def backward(self, dA):\n",
    "        \"\"\"\n",
    "        バックワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        dA : 次の形のndarray, shape (batch_size, n_nodes2)\n",
    "            後ろから流れてきた勾配\n",
    "        Returns\n",
    "        ----------\n",
    "        dZ : 次の形のndarray, shape (batch_size, n_nodes1)\n",
    "            前に流す勾配\n",
    "        \"\"\"\n",
    "        \n",
    "        grad = self._func_diff(self.input_X_forward)\n",
    "        dZ = grad * dA\n",
    "        return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 936,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class softmax:\n",
    "    \"\"\"\n",
    "    活性化関数 : Sigmoid\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_nodes1 : int\n",
    "      前の層のノード数\n",
    "    n_nodes2 : int\n",
    "      後の層のノード数\n",
    "    initializer : 初期化方法のインスタンス\n",
    "    optimizer : 最適化手法のインスタンス\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        # 初期化\n",
    "        self.input_X_forward = 0\n",
    "        self.pred = 0\n",
    "    \n",
    "    def _func(self, X):\n",
    "        #X = X - np.max(X)\n",
    "        #tmp = np.exp(X)\n",
    "        #denominator = np.sum(tmp, axis=1)\n",
    "        #output = tmp / denominator[:, np.newaxis]\n",
    "        tmp = X - np.max(X)\n",
    "        output = np.exp(tmp) / np.sum(np.exp(tmp))\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    def _func_diff(self, X):\n",
    "        return X\n",
    "        \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        フォワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (batch_size, n_nodes1)\n",
    "            入力\n",
    "        Returns\n",
    "        ----------\n",
    "        A : 次の形のndarray, shape (batch_size, n_nodes2)\n",
    "            出力\n",
    "        \"\"\" \n",
    "        \n",
    "        self.input_X_forward = X\n",
    "        A = self._func(X)\n",
    "        self.pred = A\n",
    "        return A\n",
    "    \n",
    "    def backward(self, dA):\n",
    "        \"\"\"\n",
    "        バックワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        dA : 次の形のndarray, shape (batch_size, n_nodes2)\n",
    "            後ろから流れてきた勾配\n",
    "        Returns\n",
    "        ----------\n",
    "        dZ : 次の形のndarray, shape (batch_size, n_nodes1)\n",
    "            前に流す勾配\n",
    "        \"\"\"\n",
    "        dZ = self.pred - dA\n",
    "        \n",
    "        return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 790,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ReLU:\n",
    "    \"\"\"\n",
    "    活性化関数 : Sigmoid\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_nodes1 : int\n",
    "      前の層のノード数\n",
    "    n_nodes2 : int\n",
    "      後の層のノード数\n",
    "    initializer : 初期化方法のインスタンス\n",
    "    optimizer : 最適化手法のインスタンス\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        # 初期化\n",
    "        self.input_X_forward = 0\n",
    "    \n",
    "    def _func(self, X):\n",
    "        return np.maximum(0, X)\n",
    "    \n",
    "    def _func_diff(self, X):\n",
    "        return np.where( X > 0, 1, 0)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        フォワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (batch_size, n_nodes1)\n",
    "            入力\n",
    "        Returns\n",
    "        ----------\n",
    "        A : 次の形のndarray, shape (batch_size, n_nodes2)\n",
    "            出力\n",
    "        \"\"\" \n",
    "        self.input_X_forward = X\n",
    "        A = self._func(X)\n",
    "        return A\n",
    "    \n",
    "    def backward(self, dA):\n",
    "        \"\"\"\n",
    "        バックワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        dA : 次の形のndarray, shape (batch_size, n_nodes2)\n",
    "            後ろから流れてきた勾配\n",
    "        Returns\n",
    "        ----------\n",
    "        dZ : 次の形のndarray, shape (batch_size, n_nodes1)\n",
    "            前に流す勾配\n",
    "        \"\"\"\n",
    "        \n",
    "        grad = self._func_diff(self.input_X_forward)\n",
    "        dZ = grad * dA\n",
    "        return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 791,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class XavierInitializer:\n",
    "    \"\"\"\n",
    "    Xavierによる初期化\n",
    "    Parameters\n",
    "    ----------\n",
    "    sigma : float\n",
    "      ガウス分布の標準偏差\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.n_prev_nodes = 1\n",
    "        pass\n",
    "        \n",
    "    def W(self, n_nodes1, n_nodes2):\n",
    "        self.n_prev_nodes = n_nodes1\n",
    "        W = np.random.randn(n_nodes1, n_nodes2) / np.sqrt(n_nodes1)\n",
    "        return W\n",
    "    \n",
    "    def B(self, n_nodes2):\n",
    "        B = np.random.randn(1, n_nodes2) / np.sqrt(self.n_prev_nodes)\n",
    "        return B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 792,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class HeInitializer:\n",
    "    \"\"\"\n",
    "    Heによる初期化\n",
    "    Parameters\n",
    "    ----------\n",
    "    sigma : float\n",
    "      ガウス分布の標準偏差\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.n_prev_nodes = 1\n",
    "        pass\n",
    "        \n",
    "    def W(self, n_nodes1, n_nodes2):\n",
    "        self.n_prev_nodes = n_nodes1\n",
    "        W = np.random.randn(n_nodes1, n_nodes2) * np.sqrt(2 / n_nodes1)\n",
    "        return W\n",
    "    \n",
    "    def B(self, n_nodes2):\n",
    "        B = np.random.randn(1, n_nodes2) * np.sqrt(2 / self.n_prev_nodes)\n",
    "        return B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 793,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class AdaGrad:\n",
    "    \"\"\"\n",
    "    確率的勾配降下法\n",
    "    Parameters\n",
    "    ----------\n",
    "    lr : 学習率\n",
    "    \"\"\"\n",
    "    def __init__(self, lr):\n",
    "        self.lr = lr\n",
    "        self.H_B = 1\n",
    "        self.H_W = 1\n",
    "    def update(self, layer):\n",
    "        \"\"\"\n",
    "        ある層の重みやバイアスの更新\n",
    "        Parameters\n",
    "        ----------\n",
    "        layer : 更新前の層のインスタンス\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        layer : 更新後の層のインスタンス\n",
    "        \"\"\"\n",
    "        \n",
    "        #dA, dWを更新＆保存\n",
    "        self.H_B = self.H_B + np.average(layer.dA)**2\n",
    "        self.H_W = self.H_W + np.average(layer.dW)**2\n",
    "        \n",
    "        layer.B = layer.B - self.lr * np.average(layer.dA, axis=0) / np.sqrt(self.H_B)\n",
    "        layer.W = layer.W - self.lr * layer.dW / layer.dA.shape[0] / np.sqrt(self.H_W)\n",
    "        \n",
    "        return layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 【問題1】2次元畳み込み層の作成\n",
    "Sprint12で作成した1次元畳み込み層を発展させ、2次元畳み込み層のクラスConv2dを作成してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 928,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Conv2d():\n",
    "    \"\"\"\n",
    "    2次元Convolution層\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_input_hight : 入力２次元データの高さ\n",
    "    n_input_width : 入力２次元データの幅\n",
    "    f_w : フィルタ\n",
    "    f_b :　バイアス\n",
    "    initializer : 初期化方法のインスタンス\n",
    "    optimizer : 最適化手法のインスタンス\n",
    "    \"\"\"\n",
    "    def __init__(self, n_input_hight, n_input_width, f_w, f_b, initializer, optimizer):\n",
    "        self.optimizer = optimizer\n",
    "        # 初期化\n",
    "        # initializerのメソッドを使い、self.Wとself.Bを初期化する\n",
    "        self.n_input_hight = n_input_hight\n",
    "        self.n_input_width = n_input_width\n",
    "        self.W = f_w    #(n_output, n_ch, f_size_h, f_size_w)\n",
    "        self.B = f_b    #(1, n_ch, n_output)\n",
    "        self.n_output = self.W.shape[0]\n",
    "        self.n_input_ch = self.W.shape[1]\n",
    "        self.f_hight = f_w.shape[2]\n",
    "        self.f_width = f_w.shape[3]\n",
    "        self.n_output_hight = self.n_input_hight - self.f_hight + 1\n",
    "        self.n_output_width = self.n_input_width - self.f_width + 1\n",
    "        self.input_X_forward = 0\n",
    "        self.output_X_forward = np.zeros((self.W.shape[0], self.n_output_hight))\n",
    "        self.W_feedback = np.zeros_like(self.W)\n",
    "        self.B_feedback = np.zeros_like(self.B)\n",
    "        self.Z_feedback = 0\n",
    "    \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        フォワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (batch_size, n_ch, n_feature11, n_feature12)\n",
    "            入力\n",
    "        Returns\n",
    "        ----------\n",
    "        A : 次の形のndarray, shape (batch_size, n_output, n_feature21, n_feature22)\n",
    "            出力\n",
    "        \"\"\" \n",
    "        \n",
    "        self.input_X_forward = X\n",
    "        batch_size = self.input_X_forward.shape[0]\n",
    "        A = np.zeros((batch_size, self.n_output, self.n_input_ch, self.n_output_hight, self.n_output_width))\n",
    "        B = self.B[0]\n",
    "        B = B.T\n",
    "        B = B[np.newaxis]\n",
    "        #batch方向の並列計算のためaxisを追加 (Batch, ch, hight, width) = > (batch, 1, ch, hight, width)\n",
    "        X = X[:,np.newaxis]\n",
    "        for h in range(self.n_output_hight):\n",
    "            h1 = h\n",
    "            h2 = h + self.f_hight\n",
    "            for w in range(self.n_output_width):\n",
    "                w1 = w\n",
    "                w2 = w + self.f_width\n",
    "                X_seg = X[:,:,:,h1:h2,w1:w2]\n",
    "\n",
    "                #print(\"X:{} W:{}\\n\".format(X_seg.shape, self.W.shape))\n",
    "                #アダマール積 (batch, 1, ch, filter_size) * (n_output, ch, filter_size)\n",
    "                tmp = np.sum(np.sum(X_seg * self.W, axis=4), axis=3)\n",
    "                tmp = tmp + B\n",
    "                A[:,:,:,h,w] = tmp\n",
    "\n",
    "        output = np.sum(A, axis=2)\n",
    "        return output\n",
    "    \n",
    "    def backward(self, dA):\n",
    "        \"\"\"\n",
    "        バックワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        dA : 次の形のndarray, shape (batch_size, n_output, n_feature21, n_feature22)\n",
    "            後ろから流れてきた勾配\n",
    "        Returns\n",
    "        ----------\n",
    "        dZ : 次の形のndarray, shape (batch_size, n_ch, n_feature11, n_feature12)\n",
    "            前に流す勾配\n",
    "        \"\"\"\n",
    "        \n",
    "        batch_size = self.input_X_forward.shape[0]\n",
    "        \n",
    "        #Wについて\n",
    "        #X.shape (batch_size, n_input * n_output, n_feature11, n_feature12)\n",
    "        X = np.tile(self.input_X_forward, (dA.shape[1] ,1 ,1))\n",
    "        #dL.shape (n_input * n_output, n_featue2 )\n",
    "        dL = np.zeros((batch_size, X.shape[1], dA.shape[2], dA.shape[3]))\n",
    "        for i in range(self.n_output):\n",
    "            o1 = i * self.n_input_ch\n",
    "            o2 = i * self.n_input_ch + self.n_input_ch\n",
    "            tmp = dA[:,i][:,np.newaxis]\n",
    "            dL[:,o1:o2] = np.tile(tmp, (self.n_input_ch,1 ,1))\n",
    "        \n",
    "        #print(\"dL:\",dL)\n",
    "        #入力の特徴量数 - 出力の特徴量数 +1\n",
    "        loop1 = self.n_input_hight - self.n_output_hight + 1\n",
    "        loop2 = self.n_input_width - self.n_output_width + 1\n",
    "        dW_tmp = np.zeros((batch_size, X.shape[1], loop1, loop2))\n",
    "        for h in range(loop1):\n",
    "            h1 = h\n",
    "            h2 = h + self.n_output_hight\n",
    "            for w in range(loop2):\n",
    "                w1 = w\n",
    "                w2 = w + self.n_output_width\n",
    "                dX_seg = X[:,:, h1:h2, w1:w2]\n",
    "                dW_tmp[:,:,h,w] = np.sum(np.sum(dL * dX_seg, axis=3), axis=2)\n",
    "        \n",
    "        #bacth方向の平均をとる\n",
    "        dW_tmp2 = np.average(dW_tmp, axis=0)     \n",
    "        #計算結果をフィルタサイズに整形\n",
    "        for i in range(self.n_output):\n",
    "            o1 = i * self.n_input_ch\n",
    "            o2 = i * self.n_input_ch + self.n_input_ch\n",
    "            self.W_feedback[i] = dW_tmp2[o1:o2]\n",
    "\n",
    "        #Bについて\n",
    "        #(batch_size, n_output, n_feature21, n_feature22)\n",
    "        dB = np.sum(np.sum(dA, axis=3), axis=2)\n",
    "        dB = np.average(dB, axis=0) #bacth方向の平均をとる\n",
    "        for i in range(self.n_input_ch):\n",
    "            self.B_feedback[:,i] = dB\n",
    "        \n",
    "        #Zについて Output数回す\n",
    "        self.Z_feedback = np.zeros_like(self.input_X_forward)\n",
    "        for i in range(self.n_output):\n",
    "            #損失(行列)の端の処理のため、列の前後に0列を追加（フィルタサイズから計算）\n",
    "            dA_tmp = dA[:,i][:,np.newaxis,:]\n",
    "            dA_padding = np.zeros([batch_size, 1, self.f_hight-1, dA_tmp.shape[3]])\n",
    "            dA_tmp = np.concatenate((dA_tmp, dA_padding), axis=2)\n",
    "            dA_tmp = np.concatenate((dA_padding, dA_tmp), axis=2) \n",
    "            \n",
    "            dA_padding = np.zeros([batch_size, 1, dA_tmp.shape[2], self.f_width-1])\n",
    "            dA_tmp = np.concatenate((dA_tmp, dA_padding), axis=3)\n",
    "            dA_tmp = np.concatenate((dA_padding, dA_tmp), axis=3) \n",
    "            dA_tmp = np.tile(dA_tmp, (self.n_input_ch ,1))\n",
    "            dZ_seg = np.zeros_like(self.Z_feedback)\n",
    "            \n",
    "            for h in range(self.n_input_hight):\n",
    "                h1 = h\n",
    "                h2 = h + self.f_hight\n",
    "                for w in range(self.n_input_width):\n",
    "                    w1 = w\n",
    "                    w2 = w + self.f_width\n",
    "                    \n",
    "                    dA_seg = dA_tmp[:,:,h1:h2, w1:w2]\n",
    "                    #並列計算工夫\n",
    "                    dA_seg = np.fliplr(np.fliplr(dA_seg).T).T\n",
    "                    #dA_seg = dA_seg[:,np.newaxis]\n",
    "                    tmp = np.sum(np.sum(dA_seg * self.W[i], axis=3), axis=2)\n",
    "                    #print(\"tmp.shape:\",tmp.shape)\n",
    "                    #print(\"dZ.shape:\",dZ_seg.shape)\n",
    "                    dZ_seg[:,:,h,w] = tmp\n",
    "                \n",
    "            self.Z_feedback += dZ_seg #出力数分足し算\n",
    "\n",
    "        # 更新\n",
    "        self = self.optimizer.update(self)\n",
    "        return self.Z_feedback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 929,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#初期化、更新インスタンスを作る\n",
    "optimizer = SGD(0.01)\n",
    "initializer = XavierInitializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 930,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "w = np.ones([2,2,2,2])\n",
    "b = np.ones([1,2,2])\n",
    "A = np.random.randint(0,10,(1,2,6,8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 931,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Cov = Conv2d(6,8,w,b,initializer,optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 932,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dA = Cov.forward(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 933,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[8, 4, 5, 3, 4, 2, 2, 7],\n",
       "         [9, 5, 2, 1, 4, 4, 1, 8],\n",
       "         [9, 2, 5, 8, 2, 9, 4, 9],\n",
       "         [6, 1, 2, 6, 2, 0, 0, 6],\n",
       "         [9, 6, 2, 3, 2, 0, 9, 6],\n",
       "         [1, 4, 4, 2, 5, 2, 3, 2]],\n",
       "\n",
       "        [[3, 8, 2, 4, 7, 3, 3, 3],\n",
       "         [6, 2, 5, 8, 7, 3, 3, 7],\n",
       "         [2, 2, 0, 7, 0, 9, 5, 4],\n",
       "         [8, 6, 4, 1, 6, 5, 8, 7],\n",
       "         [4, 2, 2, 9, 0, 1, 4, 9],\n",
       "         [4, 3, 0, 5, 8, 8, 7, 2]]]])"
      ]
     },
     "execution_count": 933,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 934,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[47., 35., 32., 40., 36., 23., 36.],\n",
       "         [39., 25., 38., 39., 40., 40., 43.],\n",
       "         [38., 24., 35., 34., 35., 42., 45.],\n",
       "         [44., 27., 31., 31., 18., 29., 51.],\n",
       "         [35., 25., 29., 36., 28., 36., 44.]],\n",
       "\n",
       "        [[47., 35., 32., 40., 36., 23., 36.],\n",
       "         [39., 25., 38., 39., 40., 40., 43.],\n",
       "         [38., 24., 35., 34., 35., 42., 45.],\n",
       "         [44., 27., 31., 31., 18., 29., 51.],\n",
       "         [35., 25., 29., 36., 28., 36., 44.]]]])"
      ]
     },
     "execution_count": 934,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 935,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[ 94, 164, 134, 144, 152, 118, 118,  72],\n",
       "         [172, 292, 260, 298, 310, 278, 284, 158],\n",
       "         [154, 252, 244, 292, 296, 314, 340, 176],\n",
       "         [164, 266, 234, 262, 236, 248, 334, 192],\n",
       "         [158, 262, 224, 254, 226, 222, 320, 190],\n",
       "         [ 70, 120, 108, 130, 128, 128, 160,  88]],\n",
       "\n",
       "        [[ 94, 164, 134, 144, 152, 118, 118,  72],\n",
       "         [172, 292, 260, 298, 310, 278, 284, 158],\n",
       "         [154, 252, 244, 292, 296, 314, 340, 176],\n",
       "         [164, 266, 234, 262, 236, 248, 334, 192],\n",
       "         [158, 262, 224, 254, 226, 222, 320, 190],\n",
       "         [ 70, 120, 108, 130, 128, 128, 160,  88]]]])"
      ]
     },
     "execution_count": 935,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Cov.backward(dA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 【問題2】2次元畳み込み後の出力サイズ\n",
    "畳み込みを行うと特徴マップのサイズが変化します。どのように変化するかは以下の数式から求められます。この計算を行う関数を作成してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cal_output_data_size(input_X, filter_W, padding_size_h, padding_size_w, stride_h, stride_w):\n",
    "    input_x_hight = input_X.shape[1]\n",
    "    input_x_width = input_X.shape[2]\n",
    "    output_ch = filter_W[0]\n",
    "    filter_w_hight = filter_W.shape[3]\n",
    "    filter_w_width = filter_W.shape[4]\n",
    "    \n",
    "    out_h = (input_x_hight + padding_size_h * 2 - filter_W_hight) / stride_h + 1\n",
    "    out_w = (input_x_width + padding_size_w * 2 - filter_W_width) / stride_w + 1\n",
    "    \n",
    "    return out_ch, out_h, out_w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 【問題3】最大プーリング層の作成\n",
    "最大プーリング層のクラスMaxPool2Dを作成してください。プーリング層は数式で表さない方が分かりやすい部分もありますが、フォワードプロパゲーションの数式は以下のようになります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 571,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Max_pooling():\n",
    "    \n",
    "    def __init__(self, stride_h, stride_w):\n",
    "        self.h = stride_h\n",
    "        self.w = stride_w\n",
    "        self.max_pos = 0\n",
    "        self.backward_map = 0\n",
    "        \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        X.shape (batch_size, ch, h, w)\n",
    "        \"\"\"\n",
    "        batch_size = X.shape[0]\n",
    "        ch_size = X.shape[1]\n",
    "        h_size = X.shape[2]\n",
    "        w_size = X.shape[3]\n",
    "        \n",
    "        output_size_h = (int)(h_size / self.h) \n",
    "        output_size_w = (int)(w_size / self.w)\n",
    "        output = np.zeros((batch_size, ch_size, output_size_h, output_size_w))\n",
    "        self.backward_map = np.zeros((batch_size, ch_size, output_size_h, output_size_w, self.h, self.w))\n",
    "        \n",
    "        #print(\"input:\\n\",X)\n",
    "        for n_h in range(output_size_h):\n",
    "            for n_w in range(output_size_w):\n",
    "                pos_h1 = n_h + n_h * (self.h - 1)\n",
    "                pos_h2 = pos_h1 + self.h\n",
    "                pos_w1 = n_w + n_w * (self.w - 1)\n",
    "                pos_w2 = pos_w1 + self.w\n",
    "\n",
    "                tmp = np.max(np.max(X[:,:, pos_h1:pos_h2, pos_w1:pos_w2], axis=3), axis=2)\n",
    "                #tmp = np.max(tmp, axis=2)\n",
    "                output[:,:, n_h, n_w] = tmp\n",
    "                tmp = tmp[:,:,np.newaxis,np.newaxis]\n",
    "                self.backward_map[:,:, n_h, n_w] = (X[:,:, pos_h1:pos_h2, pos_w1:pos_w2] == tmp)\n",
    "                \n",
    "        #print(\"T/F:\",self.backward_map)\n",
    "        self.backward_map = self.backward_map.astype(int)\n",
    "        \n",
    "        return output\n",
    "        \n",
    "    def backward(self, dA):\n",
    "        \"\"\"\n",
    "        dA.shape (batch_size, ch, h, w)\n",
    "        \"\"\"        \n",
    "        batch_size = dA.shape[0]\n",
    "        ch_size = dA.shape[1]\n",
    "        h_size = dA.shape[2]\n",
    "        w_size = dA.shape[3]\n",
    "        \n",
    "        output_size_h = h_size * self.h\n",
    "        output_size_w = w_size * self.w\n",
    "        output = np.zeros((batch_size, ch_size, output_size_h, output_size_w))\n",
    "        for n_h in range(h_size):\n",
    "            for n_w in range(w_size):\n",
    "                pos_h1 = n_h + n_h * (self.h - 1)\n",
    "                pos_h2 = pos_h1 + self.h\n",
    "                pos_w1 = n_w + n_w * (self.w - 1)\n",
    "                pos_w2 = pos_w1 + self.w                    \n",
    "\n",
    "                #print(\"dA:\\n\", dA[:,:, n_h, n_w])\n",
    "                #print(\"back:\\n\", self.backward_map[:,:, n_h, n_w])                    \n",
    "                tmp = dA[:,:, n_h, n_w][:,:, np.newaxis, np.newaxis]\n",
    "                output[:,:, pos_h1:pos_h2, pos_w1:pos_w2] = tmp * self.backward_map[:,:, n_h, n_w]\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 572,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p = Max_pooling(2,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 573,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "A = np.random.randint(0,10,(1,3,6,8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 574,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dA = p.forward(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 575,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input:\n",
      " [[[[2 7 8 8 0 5 3 5]\n",
      "   [3 8 4 8 2 6 1 1]\n",
      "   [3 6 1 1 2 3 3 4]\n",
      "   [3 2 1 6 7 6 7 9]\n",
      "   [6 1 6 0 9 3 7 8]\n",
      "   [8 4 6 6 6 3 6 0]]\n",
      "\n",
      "  [[5 7 5 8 9 0 9 6]\n",
      "   [5 9 0 4 9 0 3 0]\n",
      "   [0 6 4 9 3 6 3 5]\n",
      "   [8 1 9 1 3 3 2 8]\n",
      "   [3 6 7 5 4 6 9 4]\n",
      "   [6 1 4 1 2 3 7 9]]\n",
      "\n",
      "  [[6 0 5 4 7 1 0 5]\n",
      "   [7 5 8 3 7 0 2 0]\n",
      "   [1 2 4 2 2 3 9 2]\n",
      "   [3 0 1 1 5 6 3 4]\n",
      "   [6 8 3 4 5 0 8 9]\n",
      "   [1 3 2 0 9 4 8 9]]]]\n",
      "output:\n",
      " [[[[8. 8. 6. 5.]\n",
      "   [6. 6. 7. 9.]\n",
      "   [8. 6. 9. 8.]]\n",
      "\n",
      "  [[9. 8. 9. 9.]\n",
      "   [8. 9. 6. 8.]\n",
      "   [6. 7. 6. 9.]]\n",
      "\n",
      "  [[7. 8. 7. 5.]\n",
      "   [3. 4. 6. 9.]\n",
      "   [8. 4. 9. 9.]]]]\n"
     ]
    }
   ],
   "source": [
    "print(\"input:\\n\",A)\n",
    "print(\"output:\\n\",dA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 576,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[0., 0., 8., 8., 0., 0., 0., 5.],\n",
       "         [0., 8., 0., 8., 0., 6., 0., 0.],\n",
       "         [0., 6., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 6., 7., 0., 0., 9.],\n",
       "         [0., 0., 6., 0., 9., 0., 0., 8.],\n",
       "         [8., 0., 6., 6., 0., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., 8., 9., 0., 9., 0.],\n",
       "         [0., 9., 0., 0., 9., 0., 0., 0.],\n",
       "         [0., 0., 0., 9., 0., 6., 0., 0.],\n",
       "         [8., 0., 9., 0., 0., 0., 0., 8.],\n",
       "         [0., 6., 7., 0., 0., 6., 9., 0.],\n",
       "         [6., 0., 0., 0., 0., 0., 0., 9.]],\n",
       "\n",
       "        [[0., 0., 0., 0., 7., 0., 0., 5.],\n",
       "         [7., 0., 8., 0., 7., 0., 0., 0.],\n",
       "         [0., 0., 4., 0., 0., 0., 9., 0.],\n",
       "         [3., 0., 0., 0., 0., 6., 0., 0.],\n",
       "         [0., 8., 0., 4., 0., 0., 0., 9.],\n",
       "         [0., 0., 0., 0., 9., 0., 0., 9.]]]])"
      ]
     },
     "execution_count": 576,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p.backward(dA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 【問題4】平滑化\n",
    "平滑化するためのクラスFlatten()を作成してください。\n",
    "フォワードのときはチャンネル、高さ、幅の3次元を1次元にreshapeします。その値は記録しておき、バックワードのときに再びreshapeによって形を戻します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 784,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Flatten2():\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.input_X_shape = 0\n",
    "        \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        X.shape (batch_size, n_output, n_feature1, n_feature2)\n",
    "        \n",
    "        return (batch_size, n_output * n_feature1 * n_feature2)\n",
    "        \"\"\"\n",
    "        self.inout_X_shape = X.shape\n",
    "        #print(\"Flatten input x shape:\",X.shape)\n",
    "        output = X.reshape([self.inout_X_shape[0], self.inout_X_shape[1] * self.inout_X_shape[2] * self.inout_X_shape[3]])\n",
    "        return output\n",
    "    \n",
    "    def backward(self, X):\n",
    "        output = X.reshape(self.inout_X_shape)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 【問題5】学習・推定\n",
    "作成したConv2dを使用してMNISTの分類を学習・推定してください。\n",
    "この段階では精度は気にせず、動くことを確認してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 911,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "default_dnn_design = {\n",
    "    'learning_rate':0.001,\n",
    "    'total_layer':3,\n",
    "    'func_layer1':'tanh',\n",
    "    'func_layer2':'tanh',\n",
    "    'func_layer3':'softmax',\n",
    "    'node_layer0':786, \n",
    "    'node_layer1':400,\n",
    "    'node_layer2':200,\n",
    "    'node_layer3':10,\n",
    "    'initializer':'SimpleInitializer',\n",
    "    'initializer_sigma':0.05,\n",
    "    'optimizer':'SGD',\n",
    "}\n",
    "\n",
    "class ScratchDeepNeuralNetrowkClassifier2():\n",
    "    \"\"\"\n",
    "    ディープニューラルネットワーク分類器\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_epoch, batch_size, verbose = False):\n",
    "        self.verbose = verbose\n",
    "        self.batch_size = batch_size\n",
    "        self.n_epoch = n_epoch\n",
    "        self.loss = 0\n",
    "        self.loss_val = 0\n",
    "        self.activation_func = 0\n",
    "        self.affine_func = 0\n",
    "        self.n_layer = 0\n",
    "        self.layer_instance = [0 for _ in range(64)]\n",
    "        #self.activation_func = [0 for _ in range(self.dnn_design.get('total_layer'))]\n",
    "        #self.affine_func = [0 for _ in range(self.dnn_design.get('total_layer'))]\n",
    "        #self.n_layer = self.dnn_design.get('total_layer')\n",
    "        \n",
    "        #各インスタンスを生成\n",
    "        #initializerインスタンス\n",
    "        \n",
    "    def _crossentropy(self, y_pred, y):\n",
    "        #クロスエントロピーを計算する\n",
    "        INF_AVOIDANCE = 1e-8\n",
    "        cross_entropy = -1 * y * np.log(y_pred + INF_AVOIDANCE)\n",
    "        return np.sum(cross_entropy, axis=1)\n",
    "    \n",
    "    def add_layer(self, model):\n",
    "        self.layer_instance[self.n_layer] = model\n",
    "        self.n_layer += 1\n",
    "        return\n",
    "    \n",
    "    def delet_all_layer(self):\n",
    "        #add_layerでセットしたlayer情報を全てクリアする\n",
    "        self.layer_instance[0:self.n_layer] = 0\n",
    "        self.n_layer = 0\n",
    "        \n",
    "        return\n",
    "    \n",
    "    def fit(self, X, y, X_val=None, y_val=None):\n",
    "        #lossの記録用の配列を用意\n",
    "        self.loss = [[0 for i in range(X.shape[0])] for j in range(self.n_epoch)]\n",
    "        self.loss_val = [[0 for i in range(X.shape[0])] for j in range(self.n_epoch)]\n",
    "        \n",
    "        i = 0\n",
    "        get_mini_batch = GetMiniBatch(x_train, y_train, self.batch_size)\n",
    "        for epoch in range(self.n_epoch):\n",
    "            #print(\"Proceeding Epoch:\", i+1)\n",
    "            loop_count = 0\n",
    "            sum_loss = 0\n",
    "            for mini_X_train, mini_y_train in get_mini_batch:\n",
    "                X = mini_X_train\n",
    "                #Forwardの計算\n",
    "                for layer in range(self.n_layer):\n",
    "                    #print(\"Cal forward layer:{}\".format(layer))\n",
    "                    #X = self.affine_func[layer].forward(X)\n",
    "                    #X = self.activation_func[layer].forward(X)\n",
    "                    X = self.layer_instance[layer].forward(X)\n",
    "                    #print(\"layer:{} X:\\n{}\".format(layer, X))\n",
    "                \n",
    "                #Loss計算\n",
    "                #print(\"X.shape:{} Y.shape:{}\".format(X.shape, mini_y_train.shape))\n",
    "                sum_loss += self._crossentropy(X, mini_y_train)\n",
    "                    \n",
    "                #Backwardの計算\n",
    "                dz = mini_y_train\n",
    "                for layer in reversed(range(0, self.n_layer)):\n",
    "                    #print(\"Cal backward layer:{}\".format(layer))\n",
    "                    #dz = self.activation_func[layer].backward(dz)\n",
    "                    #dz = self.affine_func[layer].backward(dz)\n",
    "                    dz = self.layer_instance[layer].backward(dz)\n",
    "                    #print(\"layer:{} dz:\\n{}\".format(layer, dz))\n",
    "                \n",
    "                loop_count += 1\n",
    "                \n",
    "            #Epoch毎のLoss計算結果表示\n",
    "            self.loss[i] = sum_loss / loop_count\n",
    "            if X_val is not None and y_val is not None:\n",
    "                y_val_pred = self._predict(X_val)\n",
    "                self.loss_val[i] = self._crossentropy(y_val_pred, y_val)\n",
    "                \n",
    "            if self.verbose:\n",
    "                #verboseをTrueにした際は学習過程などを出力する\n",
    "                print(\"Epoch:{} \\n Loss:\\n{} Loss(val):\\n{}\".format(i+1, self.loss[i], self.loss_val[i]))\n",
    "                \n",
    "            i +=1\n",
    "            \n",
    "        return\n",
    "    \n",
    "    def predict(self, X):\n",
    "        #Forwardの計算\n",
    "        for layer in range(self.n_layer):\n",
    "            #X = self.affine_func[layer].forward(X)\n",
    "            #X = self.activation_func[layer].forward(X)\n",
    "            X = self.layer_instance[layer].forward(X)\n",
    "        \n",
    "        max_val = np.max(X, axis=1)\n",
    "        mask = np.ones_like(X)\n",
    "        X[X == max_val[:,np.newaxis]] = 1\n",
    "        X[X != mask] = 0        \n",
    "        \n",
    "        return X\n",
    "\n",
    "    def _predict(self, X):\n",
    "        #Forwardの計算\n",
    "        for layer in range(self.n_layer):\n",
    "            #X = self.affine_func[layer].forward(X)\n",
    "            #X = self.activation_func[layer].forward(X)  \n",
    "            X = self.layer_instance[layer].forward(X)\n",
    "        \n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 912,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Conv2d():\n",
    "    \"\"\"\n",
    "    2次元Convolution層\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_input_hight : 入力２次元データの高さ\n",
    "    n_input_width : 入力２次元データの幅\n",
    "    f_w : フィルタ\n",
    "    f_b :　バイアス\n",
    "    initializer : 初期化方法のインスタンス\n",
    "    optimizer : 最適化手法のインスタンス\n",
    "    \"\"\"\n",
    "    def __init__(self, n_input_hight, n_input_width, f_w, f_b, initializer, optimizer):\n",
    "        self.optimizer = optimizer\n",
    "        # 初期化\n",
    "        # initializerのメソッドを使い、self.Wとself.Bを初期化する\n",
    "        self.n_input_hight = n_input_hight\n",
    "        self.n_input_width = n_input_width\n",
    "        self.W = f_w    #(n_output, n_ch, f_size_h, f_size_w)\n",
    "        self.B = f_b    #(1, n_ch, n_output)\n",
    "        self.n_output = self.W.shape[0]\n",
    "        self.n_input_ch = self.W.shape[1]\n",
    "        self.f_hight = f_w.shape[2]\n",
    "        self.f_width = f_w.shape[3]\n",
    "        self.n_output_hight = self.n_input_hight - self.f_hight + 1\n",
    "        self.n_output_width = self.n_input_width - self.f_width + 1\n",
    "        self.input_X_forward = 0\n",
    "        self.output_X_forward = np.zeros((self.W.shape[0], self.n_output_hight))\n",
    "        self.W_feedback = np.zeros_like(self.W)\n",
    "        self.B_feedback = np.zeros_like(self.B)\n",
    "        self.Z_feedback = 0\n",
    "    \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        フォワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (batch_size, n_ch, n_feature11, n_feature12)\n",
    "            入力\n",
    "        Returns\n",
    "        ----------\n",
    "        A : 次の形のndarray, shape (batch_size, n_output, n_feature21, n_feature22)\n",
    "            出力\n",
    "        \"\"\" \n",
    "        \n",
    "        self.input_X_forward = X\n",
    "        batch_size = self.input_X_forward.shape[0]\n",
    "        A = np.zeros((batch_size, self.n_output, self.n_input_ch, self.n_output_hight, self.n_output_width))\n",
    "        B = self.B[0]\n",
    "        B = B.T\n",
    "        B = B[np.newaxis]\n",
    "        #batch方向の並列計算のためaxisを追加 (Batch, ch, hight, width) = > (batch, 1, ch, hight, width)\n",
    "        X = X[:,np.newaxis]\n",
    "        for h in range(self.n_output_hight):\n",
    "            h1 = h\n",
    "            h2 = h + self.f_hight\n",
    "            for w in range(self.n_output_width):\n",
    "                w1 = w\n",
    "                w2 = w + self.f_width\n",
    "                X_seg = X[:,:,:,h1:h2,w1:w2]\n",
    "\n",
    "                #print(\"X:{} W:{}\\n\".format(X_seg.shape, self.W.shape))\n",
    "                #アダマール積 (batch, 1, ch, filter_size) * (n_output, ch, filter_size)\n",
    "                tmp = np.sum(np.sum(X_seg * self.W, axis=4), axis=3)\n",
    "                tmp = tmp + B\n",
    "                A[:,:,:,h,w] = tmp\n",
    "\n",
    "        output = np.sum(A, axis=2)\n",
    "        return output\n",
    "    \n",
    "    def backward(self, dA):\n",
    "        \"\"\"\n",
    "        バックワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        dA : 次の形のndarray, shape (batch_size, n_output, n_feature21, n_feature22)\n",
    "            後ろから流れてきた勾配\n",
    "        Returns\n",
    "        ----------\n",
    "        dZ : 次の形のndarray, shape (batch_size, n_ch, n_feature11, n_feature12)\n",
    "            前に流す勾配\n",
    "        \"\"\"\n",
    "        \n",
    "        batch_size = self.input_X_forward.shape[0]\n",
    "        \n",
    "        #Wについて\n",
    "        #X.shape (batch_size, n_input * n_output, n_feature11, n_feature12)\n",
    "        X = np.tile(self.input_X_forward, (dA.shape[1] ,1 ,1))\n",
    "        #dL.shape (n_input * n_output, n_featue2 )\n",
    "        dL = np.zeros((batch_size, X.shape[1], dA.shape[2], dA.shape[3]))\n",
    "        for i in range(self.n_output):\n",
    "            o1 = i * self.n_input_ch\n",
    "            o2 = i * self.n_input_ch + self.n_input_ch\n",
    "            tmp = dA[:,i][:,np.newaxis]\n",
    "            dL[:,o1:o2] = np.tile(tmp, (self.n_input_ch,1 ,1))\n",
    "        \n",
    "        #print(\"dL:\",dL)\n",
    "        #入力の特徴量数 - 出力の特徴量数 +1\n",
    "        loop1 = self.n_input_hight - self.n_output_hight + 1\n",
    "        loop2 = self.n_input_width - self.n_output_width + 1\n",
    "        dW_tmp = np.zeros((batch_size, X.shape[1], loop1, loop2))\n",
    "        for h in range(loop1):\n",
    "            h1 = h\n",
    "            h2 = h + self.n_output_hight\n",
    "            for w in range(loop2):\n",
    "                w1 = w\n",
    "                w2 = w + self.n_output_width\n",
    "                dX_seg = X[:,:, h1:h2, w1:w2]\n",
    "                dW_tmp[:,:,h,w] = np.sum(np.sum(dL * dX_seg, axis=3), axis=2)\n",
    "        \n",
    "        #bacth方向の平均をとる\n",
    "        dW_tmp2 = np.average(dW_tmp, axis=0)     \n",
    "        #計算結果をフィルタサイズに整形\n",
    "        for i in range(self.n_output):\n",
    "            o1 = i * self.n_input_ch\n",
    "            o2 = i * self.n_input_ch + self.n_input_ch\n",
    "            self.W_feedback[i] = dW_tmp2[o1:o2]\n",
    "\n",
    "        #Bについて\n",
    "        #(batch_size, n_output, n_feature21, n_feature22)\n",
    "        dB = np.sum(np.sum(dA, axis=3), axis=2)\n",
    "        dB = np.average(dB, axis=0) #bacth方向の平均をとる\n",
    "        for i in range(self.n_input_ch):\n",
    "            self.B_feedback[:,i] = dB\n",
    "        \n",
    "        #Zについて Output数回す\n",
    "        self.Z_feedback = np.zeros_like(self.input_X_forward)\n",
    "        for i in range(self.n_output):\n",
    "            #損失(行列)の端の処理のため、列の前後に0列を追加（フィルタサイズから計算）\n",
    "            dA_tmp = dA[:,i][:,np.newaxis,:]\n",
    "            dA_padding = np.zeros([batch_size, 1, self.f_hight-1, dA_tmp.shape[3]])\n",
    "            dA_tmp = np.concatenate((dA_tmp, dA_padding), axis=2)\n",
    "            dA_tmp = np.concatenate((dA_padding, dA_tmp), axis=2) \n",
    "            \n",
    "            dA_padding = np.zeros([batch_size, 1, dA_tmp.shape[2], self.f_width-1])\n",
    "            dA_tmp = np.concatenate((dA_tmp, dA_padding), axis=3)\n",
    "            dA_tmp = np.concatenate((dA_padding, dA_tmp), axis=3) \n",
    "            dA_tmp = np.tile(dA_tmp, (self.n_input_ch ,1))\n",
    "            dZ_seg = np.zeros_like(self.Z_feedback)\n",
    "            \n",
    "            for h in range(self.n_input_hight):\n",
    "                h1 = h\n",
    "                h2 = h + self.f_hight\n",
    "                for w in range(self.n_input_width):\n",
    "                    w1 = w\n",
    "                    w2 = w + self.f_width\n",
    "                    \n",
    "                    dA_seg = dA_tmp[:,:,h1:h2, w1:w2]\n",
    "                    #並列計算工夫\n",
    "                    dA_seg = np.fliplr(np.fliplr(dA_seg).T).T\n",
    "                    #dA_seg = dA_seg[:,np.newaxis]\n",
    "                    tmp = np.sum(np.sum(dA_seg * self.W[i], axis=3), axis=2)\n",
    "                    #print(\"tmp.shape:\",tmp.shape)\n",
    "                    #print(\"dZ.shape:\",dZ_seg.shape)\n",
    "                    dZ_seg[:,:,h,w] = tmp\n",
    "                \n",
    "            self.Z_feedback += dZ_seg #出力数分足し算\n",
    "\n",
    "        # 更新\n",
    "        self = self.optimizer.update(self)\n",
    "        return self.Z_feedback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 862,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3000, 1, 28, 28)\n",
      "(57000, 1, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import mnist\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "#x_train = x_train.reshape(-1, 784)\n",
    "#x_test = x_test.reshape(-1, 784)\n",
    "\n",
    "#0~255を0~1スケールへ\n",
    "x_train = x_train.astype(np.float)\n",
    "x_test = x_test.astype(np.float)\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "\n",
    "#0~9をone-hot encoding\n",
    "enc = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "y_train_one_hot = enc.fit_transform(y_train[:, np.newaxis])\n",
    "y_test_one_hot = enc.transform(y_test[:, np.newaxis])\n",
    "\n",
    "#chを追加\n",
    "x_train = x_train[:,np.newaxis,:]\n",
    "x_test = x_test[:,np.newaxis,:]\n",
    "\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train_one_hot, test_size=0.95)\n",
    "print(x_train.shape) # (48000, 784)\n",
    "print(x_val.shape) # (12000, 784)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 904,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "w = np.random.randn(2,1,2,2)\n",
    "b = np.random.randn(1,1,2)\n",
    "#A = np.random.randint(0,10,(1,2,6,8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 721,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "CNN2 = ScratchDeepNeuralNetrowkClassifier2(10, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 722,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#DNNデザイン\n",
    "CNN2.add_layer(Conv2d(x_train.shape[2],x_train.shape[3], w, b, initializer, optimizer))\n",
    "CNN2.add_layer(Flatten2())\n",
    "CNN2.add_layer(FC2(w.shape[0] * (x_train.shape[2] - w.shape[2] + 1) * (x_train.shape[3] - w.shape[3] + 1), 100, initializer, optimizer))\n",
    "CNN2.add_layer(Sigmoid())\n",
    "CNN2.add_layer(FC2(100, 10, initializer, optimizer))\n",
    "CNN2.add_layer(softmax())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 723,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "CNN2.fit(x_train, y_train, x_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 724,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred = CNN2.predict(x_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 729,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pred=\n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "Yval=\n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Pred=\\n\", y_pred)\n",
    "print(\"Yval=\\n\", y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 726,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score=0.902\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy score={:.3f}\".format(accuracy_score(y_pred, y_val)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 727,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRAr8PwfZbeDV1riUAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1374dc160>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "loss = np.array(CNN2.loss)\n",
    "loss_ave = np.average(loss, axis=1)\n",
    "\n",
    "loss_val = np.array(CNN2.loss_val)\n",
    "loss_val_ave = np.average(loss_val, axis=1)\n",
    "\n",
    "plt.title(\"epoch vs loss\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.plot(loss_ave, \"r\", label=\"loss\")\n",
    "plt.plot(loss_val_ave, \"b\", label=\"val_loss\")\n",
    "plt.legend()\n",
    "#plt.yscale(\"Log\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 【問題6】（アドバンス課題）LeNet\n",
    "CNNで画像認識を行う際は、フィルタサイズや層の数などを１から考えるのではなく、有名な構造を利用することが一般的です。\n",
    "サブサンプリングとは現在のプーリングに相当するものです。現代風に以下のように作ってみることにします。活性化関数も当時はシグモイド関数ですが、ReLUとします。  \n",
    "\n",
    "畳み込み層　出力チャンネル数6、フィルタサイズ5×5、ストライド1  \n",
    "ReLU  \n",
    "最大プーリング  \n",
    "畳み込み層　出力チャンネル数16、フィルタサイズ5×5、ストライド1  \n",
    "ReLU  \n",
    "最大プーリング  \n",
    "平滑化  \n",
    "全結合層　出力ノード数120  \n",
    "ReLU  \n",
    "全結合層　出力ノード数84  \n",
    "ReLU  \n",
    "全結合層　出力ノード数10  \n",
    "ソフトマックス関数  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 913,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_epoch = 10\n",
    "n_batch = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 914,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "CNN3 = ScratchDeepNeuralNetrowkClassifier2(n_epoch, n_batch, verbose =True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 915,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "conv_w_lenet1 = np.random.randn(6,1,5,5)\n",
    "conv_b_lenet1 = np.random.randn(1,1,6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 916,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conv_w_lenet2 = np.random.randn(16,6,5,5)\n",
    "conv_b_lenet2 = np.random.randn(1,6,16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 917,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#DNNデザイン\n",
    "CNN3.add_layer(Conv2d(x_train.shape[2],x_train.shape[3], conv_w_lenet1, conv_b_lenet1, SimpleInitializer(), SGD(0.01)))\n",
    "CNN3.add_layer(ReLU())\n",
    "CNN3.add_layer(Max_pooling(2,2))\n",
    "CNN3.add_layer(Conv2d(12,12, conv_w_lenet2, conv_b_lenet2, SimpleInitializer(), SGD(0.01)))\n",
    "CNN3.add_layer(ReLU())\n",
    "CNN3.add_layer(Max_pooling(2,2))\n",
    "CNN3.add_layer(Flatten2())\n",
    "CNN3.add_layer(FC2(256, 120, SimpleInitializer(), SGD(0.01)))\n",
    "CNN3.add_layer(ReLU())\n",
    "CNN3.add_layer(FC2(120, 84, SimpleInitializer(), SGD(0.01)))\n",
    "CNN3.add_layer(ReLU())\n",
    "CNN3.add_layer(FC2(84, 10, SimpleInitializer(), SGD(0.01)))\n",
    "CNN3.add_layer(softmax())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 918,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0 Loss:[2.08198785 2.07380429 2.09045099 2.07036911 2.04661384 2.01075812\n",
      " 2.09455    2.12586225 2.06563038 2.03775421 2.10662619 2.08629494\n",
      " 2.10500621 2.11393354 2.02277715 2.0890891  2.0590412  2.10968905\n",
      " 2.12703713 2.05237148] Loss(val):[0.67308609 2.05978883 1.81099223 ... 2.29058466 2.55968798 0.70718972]\n",
      "Epoch:1 Loss:[1.20350211 1.07499465 1.1760097  1.22035203 1.18857693 1.17927005\n",
      " 1.29060517 1.27172893 1.29066286 1.25487038 1.12213232 1.13544966\n",
      " 1.20962599 1.22375913 1.1478295  1.11679901 1.11192286 1.19455375\n",
      " 1.23092222 1.21387682] Loss(val):[0.05075557 1.21704604 1.61724795 ... 1.50651849 1.09922481 0.06612355]\n",
      "Epoch:2 Loss:[0.65310005 0.55238612 0.62755812 0.67174243 0.70782765 0.75983207\n",
      " 0.68726609 0.7401475  0.77069295 0.77266132 0.65354079 0.59258109\n",
      " 0.76109608 0.74973971 0.63907465 0.68218    0.67750449 0.71061851\n",
      " 0.67376176 0.7325704 ] Loss(val):[0.00397843 1.14852163 1.85934415 ... 2.69091007 2.1029543  0.01141579]\n",
      "Epoch:3 Loss:[0.45663694 0.33585494 0.41517718 0.45059052 0.45271584 0.60675464\n",
      " 0.4971935  0.55138449 0.54812628 0.59104607 0.46741499 0.42817832\n",
      " 0.61877029 0.50550971 0.41517685 0.49946079 0.52825581 0.59880127\n",
      " 0.47753743 0.55404679] Loss(val):[8.45996149e-04 1.01375223e+00 6.13916757e-01 ... 2.17830106e+00\n",
      " 2.13952481e+00 2.09297674e-03]\n",
      "Epoch:4 Loss:[0.31981843 0.24663205 0.29813147 0.33637173 0.32139482 0.45832309\n",
      " 0.39645268 0.42869921 0.35535849 0.50298011 0.3470333  0.28718748\n",
      " 0.46711154 0.36504317 0.31660283 0.39663537 0.38302519 0.45041948\n",
      " 0.3184056  0.45439811] Loss(val):[6.32543625e-04 8.81893064e-01 1.48323080e-01 ... 9.66403493e-01\n",
      " 1.43851036e+00 1.69207170e-03]\n",
      "Epoch:5 Loss:[0.24285807 0.19784502 0.24666066 0.26614864 0.24197465 0.32904997\n",
      " 0.34652703 0.35639653 0.27272111 0.42660063 0.28832669 0.20621852\n",
      " 0.35552886 0.27342864 0.26355485 0.34304792 0.32086601 0.35126215\n",
      " 0.24106894 0.3969533 ] Loss(val):[7.49432670e-04 9.18333313e-01 5.54070206e-02 ... 6.52961930e-01\n",
      " 1.03220843e+00 2.11487164e-03]\n",
      "Epoch:6 Loss:[0.19312261 0.16175022 0.20618347 0.24024215 0.19522317 0.26030747\n",
      " 0.29548839 0.31210701 0.23186431 0.34425559 0.24484195 0.16705016\n",
      " 0.26602702 0.21858461 0.22733357 0.29790618 0.28624164 0.28550878\n",
      " 0.18095349 0.337211  ] Loss(val):[7.39565110e-04 8.73255831e-01 2.62135851e-02 ... 3.37326525e-01\n",
      " 7.79280171e-01 3.00395263e-03]\n",
      "Epoch:7 Loss:[0.15908429 0.14342382 0.18641205 0.20870002 0.15859918 0.22203168\n",
      " 0.24495629 0.27541003 0.20160815 0.31088458 0.20140914 0.12489927\n",
      " 0.21241982 0.1921445  0.19730276 0.23877088 0.2466614  0.22360265\n",
      " 0.14129725 0.29826151] Loss(val):[5.62796389e-04 1.04537541e+00 1.90271501e-02 ... 1.41887757e-01\n",
      " 3.60150210e-01 2.49482408e-03]\n",
      "Epoch:8 Loss:[0.13732416 0.12908964 0.15910037 0.17664699 0.13997163 0.18121841\n",
      " 0.20658492 0.25456937 0.17719145 0.27052394 0.16894598 0.10519265\n",
      " 0.1743705  0.15931077 0.16590678 0.20436639 0.2134954  0.18152143\n",
      " 0.11287138 0.262157  ] Loss(val):[3.04108311e-04 8.95268923e-01 1.98325101e-02 ... 8.80410795e-02\n",
      " 2.22867432e-01 1.55434310e-03]\n",
      "Epoch:9 Loss:[0.11560967 0.10124185 0.13482171 0.14493439 0.12226218 0.15486361\n",
      " 0.18141949 0.2325986  0.15101892 0.25159108 0.13990011 0.08341651\n",
      " 0.15405461 0.13115396 0.14165725 0.18185553 0.18012096 0.13828815\n",
      " 0.0947209  0.22927784] Loss(val):[1.83486459e-04 7.83402731e-01 1.13277341e-02 ... 3.84090056e-02\n",
      " 8.59494711e-02 1.62548030e-03]\n"
     ]
    }
   ],
   "source": [
    "CNN3.fit(x_train, y_train, x_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 815,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(57000, 1, 28, 28)"
      ]
     },
     "execution_count": 815,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 920,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_pred = CNN3.predict(x_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 921,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score=0.891\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy score={:.3f}\".format(accuracy_score(y_pred, y_val)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 919,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYwAAAEWCAYAAAB1xKBvAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nlw8AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x13fdc5be0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "loss = np.array(CNN3.loss)\n",
    "loss_ave = np.average(loss, axis=1)\n",
    "\n",
    "loss_val = np.array(CNN3.loss_val)\n",
    "loss_val_ave = np.average(loss_val, axis=1)\n",
    "\n",
    "plt.title(\"epoch vs loss\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.plot(loss_ave, \"r\", label=\"loss\")\n",
    "plt.plot(loss_val_ave, \"b\", label=\"val_loss\")\n",
    "plt.legend()\n",
    "#plt.yscale(\"Log\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 【問題7】（アドバンス課題）有名な画像認識モデルの調査\n",
    "CNNの代表的な構造としてははAlexNet(2012)、VGG16(2014)などがあります。こういったものはフレームワークで既に用意されていることも多いです。  \n",
    "どういったものがあるか簡単に調べてまとめてください。名前だけでも見ておくと良いでしょう。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 1. AlexNet  \n",
    "畳み込み層を5層重ねつつ，pooling層で特徴マップを縮小し，その後，3層の全結合層により最終的な出力を得る．基本的なアーキテクチャの設計思想はNeocognitronやLeNetを踏襲している  \n",
    "\n",
    "###### 2. ZFNet  \n",
    "AlexNetの最初の畳み込み層のフィルタサイズを11から7に縮小＆strideを4から2に縮小したモデル  \n",
    "\n",
    "###### 3. ResNet  \n",
    "処理ブロックへの入力xをショートカットし，H(x)=F(x)+xを次の層に渡す。  \n",
    "勾配消失を緩和することができるためより深い層を作ることができる。\n",
    "\n",
    "###### 4. GoogleNet\n",
    "複数の畳み込み層やpooling層から構成されるInceptionモジュールと呼ばれる小さなネットワーク (micro networks) を定義し，これを通常の畳み込み層のように重ねていくことで1つの大きなCNNを作り上げる\n",
    "\n",
    "###### 5. SENet\n",
    "特徴マップをチャネル毎に適応的に重み付けするAttentionの機構を導入したネットワークである  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 【問題8】（アドバンス課題）平均プーリングの作成\n",
    "平均プーリング層のクラスAveragePool2Dを作成してください。\n",
    "範囲内の最大値ではなく、平均値を出力とするプーリング層です。\n",
    "画像認識関係では最大プーリング層が一般的で、平均プーリングはあまり使われません。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Max_pooling():\n",
    "    \n",
    "    def __init__(self, stride_h, stride_w):\n",
    "        self.h = stride_h\n",
    "        self.w = stride_w\n",
    "        self.max_pos = 0\n",
    "        self.backward_map = 0\n",
    "        \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        X.shape (batch_size, ch, h, w)\n",
    "        \"\"\"\n",
    "        batch_size = X.shape[0]\n",
    "        ch_size = X.shape[1]\n",
    "        h_size = X.shape[2]\n",
    "        w_size = X.shape[3]\n",
    "        \n",
    "        output_size_h = (int)(h_size / self.h) \n",
    "        output_size_w = (int)(w_size / self.w)\n",
    "        output = np.zeros((batch_size, ch_size, output_size_h, output_size_w))\n",
    "        self.backward_map = np.zeros((batch_size, ch_size, output_size_h, output_size_w, self.h, self.w))\n",
    "        \n",
    "        #print(\"input:\\n\",X)\n",
    "        for n_h in range(output_size_h):\n",
    "            for n_w in range(output_size_w):\n",
    "                pos_h1 = n_h + n_h * (self.h - 1)\n",
    "                pos_h2 = pos_h1 + self.h\n",
    "                pos_w1 = n_w + n_w * (self.w - 1)\n",
    "                pos_w2 = pos_w1 + self.w\n",
    "\n",
    "                tmp = np.average(np.average(X[:,:, pos_h1:pos_h2, pos_w1:pos_w2], axis=3), axis=2)\n",
    "                #tmp = np.max(tmp, axis=2)\n",
    "                output[:,:, n_h, n_w] = tmp\n",
    "                tmp = tmp[:,:,np.newaxis,np.newaxis]\n",
    "                self.backward_map[:,:, n_h, n_w] = (X[:,:, pos_h1:pos_h2, pos_w1:pos_w2] == tmp)\n",
    "                \n",
    "        #print(\"T/F:\",self.backward_map)\n",
    "        self.backward_map = self.backward_map.astype(int)\n",
    "        \n",
    "        return output\n",
    "        \n",
    "    def backward(self, dA):\n",
    "        \"\"\"\n",
    "        dA.shape (batch_size, ch, h, w)\n",
    "        \"\"\"        \n",
    "        batch_size = dA.shape[0]\n",
    "        ch_size = dA.shape[1]\n",
    "        h_size = dA.shape[2]\n",
    "        w_size = dA.shape[3]\n",
    "        \n",
    "        output_size_h = h_size * self.h\n",
    "        output_size_w = w_size * self.w\n",
    "        output = np.zeros((batch_size, ch_size, output_size_h, output_size_w))\n",
    "        for n_h in range(h_size):\n",
    "            for n_w in range(w_size):\n",
    "                pos_h1 = n_h + n_h * (self.h - 1)\n",
    "                pos_h2 = pos_h1 + self.h\n",
    "                pos_w1 = n_w + n_w * (self.w - 1)\n",
    "                pos_w2 = pos_w1 + self.w                    \n",
    "\n",
    "                #print(\"dA:\\n\", dA[:,:, n_h, n_w])\n",
    "                #print(\"back:\\n\", self.backward_map[:,:, n_h, n_w])                    \n",
    "                tmp = dA[:,:, n_h, n_w][:,:, np.newaxis, np.newaxis]\n",
    "                output[:,:, pos_h1:pos_h2, pos_w1:pos_w2] = tmp * self.backward_map[:,:, n_h, n_w]\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 【問題9】出力サイズとパラメータ数の計算\n",
    "CNNモデルを構築する際には、全結合層に入力する段階で特徴量がいくつになっているかを事前に計算する必要があります。  \n",
    "また、巨大なモデルを扱うようになると、メモリや計算速度の関係でパラメータ数の計算は必須になってきます。フレームワークでは各層のパラメータ数を表示させることが可能ですが、意味を理解していなくては適切な調整が行えません。  \n",
    "以下の3つの畳み込み層の出力サイズとパラメータ数を計算してください。パラメータ数についてはバイアス項も考えてください。  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.  \n",
    "入力サイズ : 144×144, 3チャンネル  \n",
    "フィルタサイズ : 3×3, 6チャンネル  \n",
    "ストライド : 1  \n",
    "パディング : なし "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output.shape => N_output:6, H:142, W:142  \n",
    "W.shape => N_output:6 N_input_ch:3 F_H:3 F_W:3  \n",
    "B.shape => N_input_ch:3 N_output:6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.\n",
    "入力サイズ : 60×60, 24チャンネル  \n",
    "フィルタサイズ : 3×3, 48チャンネル  \n",
    "ストライド　: 1  \n",
    "パディング : なし "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output.shape => N_output:48, H:58, W:58  \n",
    "W.shape => N_output:48 N_input_ch:24 F_H:3 F_W:3  \n",
    "B.shape => N_input_ch:24 N_output:48"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.\n",
    "入力サイズ : 20×20, 10チャンネル  \n",
    "フィルタサイズ: 3×3, 20チャンネル  \n",
    "ストライド : 2  \n",
    "パディング : なし  \n",
    "＊最後の例は丁度良く畳み込みをすることができない場合です。フレームワークでは余ったピクセルを見ないという処理が行われることがあるので、その場合を考えて計算してください。端が欠けてしまうので、こういった設定は好ましくないという例です。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output.shape => N_output:20, H:8, W:8  \n",
    "W.shape => N_output:20 N_input_ch:10 F_H:3 F_W:3  \n",
    "B.shape => N_input_ch:10 N_output:20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 【問題10】（アドバンス課題）フィルタサイズに関する調査\n",
    "畳み込み層にはフィルタサイズというハイパーパラメータがありますが、2次元畳み込み層において現在では3×3と1×1の使用が大半です。以下のそれぞれを調べたり、自分なりに考えて説明してください。  \n",
    "・7×7などの大きめのものではなく、3×3のフィルタが一般的に使われる理由  \n",
    "・高さや幅方向を持たない1×1のフィルタの効果  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "1. 大きすぎるフィルタではデータの空間的な特徴が失われてしまうためより小さなフィルタが選ばれている。（と思う。。。）\n",
    "2. 入力データを適当な重みをつけてチャンネル方向に畳み込むことができる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
