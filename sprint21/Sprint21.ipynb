{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 【問題1】コードレビュー\n",
    "転移学習を使用してセグメンテーションの精度を改善したコードを提示するので、レビューを行ってください。\n",
    "視点例\n",
    "Sprint20で使用した実装とはどのように違うのか\n",
    "転移学習をどのように行っているか"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Kaggle kernel "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras import Model\n",
    "from keras.layers import (Activation, BatchNormalization, Concatenate, Conv2D,\n",
    "                          Conv2DTranspose, Dropout, Input, MaxPooling2D,\n",
    "                          UpSampling2D, concatenate)\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "U-Netのブロック\n",
    "\"\"\"\n",
    "def conv_block(m, dim, acti, bn, res, do=0):\n",
    "    n = Conv2D(dim, 3, activation=acti, padding='same')(m)\n",
    "    n = BatchNormalization()(n) if bn else n\n",
    "    n = Dropout(do)(n) if do else n\n",
    "    n = Conv2D(dim, 3, activation=acti, padding='same')(n)\n",
    "    n = BatchNormalization()(n) if bn else n\n",
    "    return Concatenate()([m, n]) if res else n\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "・Depth分だけlevel_blockを再起呼び出し\n",
    "・再起呼び出しのたびにConv_blockで定義されるネットワークを追加、Conv_blockの後にMaxpoolingかConvolutionを行う\n",
    "・depth=0に到達した後、Up-sampling+Conv2D (or Conv2DTranspose)　+ Concatenate + conv_block がDepthの数だけ追加される\n",
    "・最終層(Depth=0)はConv_block (else処理)\n",
    "\n",
    "m : Input \n",
    "dim : チャンネル方向の次元\n",
    "depth :　U-Netの深さ（同じ識別ネットワークを何回繰り返すか）\n",
    "inc :　各深さでのチャンネル方向の次元が増分（前の深さの何倍になるか）\n",
    "acti :　活性化関数\n",
    "do : Dropoutの比率\n",
    "bn : Batch normalizationの有無\n",
    "mp : Maxpoolingを使うか、Convolutionをするか \n",
    "up :　デコーダーのup samplingの方法( up-sampling+Conv2D or Conv2DTranspose )\n",
    "res : 使われてない？\n",
    "\"\"\"\n",
    "def level_block(m, dim, depth, inc, acti, do, bn, mp, up, res):\n",
    "    if depth > 0:\n",
    "        \"\"\"\n",
    "        エンコーダー\n",
    "        \"\"\"\n",
    "        n = conv_block(m, dim, acti, bn, res)\n",
    "        m = MaxPooling2D()(n) if mp else Conv2D(dim, 3, strides=2, padding='same')(n)\n",
    "        m = level_block(m, int(inc * dim), depth - 1,\n",
    "                        inc, acti, do, bn, mp, up, res)\n",
    "        \n",
    "        \"\"\"\n",
    "        デコーダー\n",
    "        \"\"\"\n",
    "        if up:\n",
    "            m = UpSampling2D()(m)\n",
    "            m = Conv2D(dim, 2, activation=acti, padding='same')(m)\n",
    "        else:\n",
    "            m = Conv2DTranspose(dim, 3, strides=2,\n",
    "                                activation=acti, padding='same')(m)\n",
    "        n = Concatenate()([n, m])\n",
    "        m = conv_block(n, dim, acti, bn, res)\n",
    "    else:\n",
    "        \"\"\"\n",
    "        最終層(depth=0)\n",
    "        \"\"\"\n",
    "        m = conv_block(m, dim, acti, bn, res, do)\n",
    "    return m\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "U-NET アーキテクチャを定義\n",
    "\"\"\"\n",
    "def UNet(params):\n",
    "\n",
    "    img_shape = params['input_dim']\n",
    "    out_ch = 1\n",
    "    start_ch = 8\n",
    "    depth = 3\n",
    "    inc_rate = 2.\n",
    "    activation = 'relu'\n",
    "    dropout = 0.5\n",
    "    batchnorm = False\n",
    "    maxpool = True\n",
    "    upconv = True\n",
    "    residual = False\n",
    "\n",
    "    \"\"\"\n",
    "    入力\n",
    "    \"\"\"\n",
    "    i = Input(shape=img_shape)\n",
    "    \"\"\"\n",
    "    U-NETコア\n",
    "    \"\"\"\n",
    "    o = level_block(i, start_ch, depth, inc_rate, activation,\n",
    "                    dropout, batchnorm, maxpool, upconv, residual)\n",
    "    \n",
    "    \"\"\"\n",
    "    出力層\n",
    "    \"\"\"\n",
    "    o = Conv2D(out_ch, 1)(o)\n",
    "    # Sigmoid activation is used because model is trained with binary_crossentropy.\n",
    "    o =  Activation('sigmoid')(o)\n",
    "\n",
    "    model = Model(inputs=i, outputs=o)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### ResNet 転移学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def unet_resnet(input_size, decoder_block,\n",
    "                weights='imagenet',\n",
    "                loss_func='binary_crossentropy',\n",
    "                metrics_list=[my_iou_metric],\n",
    "                use_lovash=False):\n",
    "\n",
    "    \"\"\"\n",
    "    エンコーダにResNetを使用\n",
    "    weights='imagenet'でImageNetの学習済み重みを設定\n",
    "    \"\"\"\n",
    "    # Base model - encoder\n",
    "    base_model = ResNet50(\n",
    "        input_shape=input_size, \n",
    "        include_top=False,\n",
    "        weights=weights)\n",
    "    \n",
    "    \"\"\"\n",
    "    Skip connectionのため対応するレイヤーの出力を取り出し\n",
    "    \"\"\"\n",
    "    # Layers for feature extraction in the encoder part\n",
    "    encoder1 = base_model.get_layer('activation_1').output\n",
    "    encoder2 = base_model.get_layer('activation_10').output\n",
    "    encoder3 = base_model.get_layer('activation_22').output\n",
    "    encoder4 = base_model.get_layer('activation_40').output\n",
    "    encoder5 = base_model.get_layer('activation_49').output\n",
    "\n",
    "    \"\"\"\n",
    "    最深部\n",
    "    \"\"\"\n",
    "    # Center block\n",
    "    center = decoder_block(\n",
    "        encoder5, 'center', num_filters=512)\n",
    "    concat5 = concatenate([center, encoder5], axis=-1) #なぞ？ Upsampling無しにconcatしている。\n",
    "\n",
    "    \"\"\"\n",
    "    デコーダ\n",
    "    \"\"\"\n",
    "    # Decoder part.\n",
    "    # Every decoder block processed concatenated output from encoder and decoder part.\n",
    "    # This creates skip connections.\n",
    "    # Afterwards, decoder output is upsampled to dimensions equal to encoder output part.\n",
    "    decoder4 = decoder_block(\n",
    "        concat5, 'decoder4', num_filters=256)\n",
    "    concat4 = concatenate([UpSampling2D()(decoder4), encoder4], axis=-1)\n",
    "\n",
    "    decoder3 = decoder_block(\n",
    "        concat4, 'decoder3', num_filters=128)\n",
    "    concat3 = concatenate([UpSampling2D()(decoder3), encoder3], axis=-1)\n",
    "\n",
    "    decoder2 = decoder_block(\n",
    "        concat3, 'decoder2', num_filters=64)\n",
    "    concat2 = concatenate([UpSampling2D()(decoder2), encoder2], axis=-1)\n",
    "\n",
    "    decoder1 = decoder_block(\n",
    "        concat2, 'decoder1', num_filters=64)\n",
    "    concat1 = concatenate([UpSampling2D()(decoder1), encoder1], axis=-1)\n",
    "\n",
    "    \"\"\"\n",
    "    出力層\n",
    "    \"\"\"\n",
    "    # Final upsampling and decoder block for segmentation.\n",
    "    output = UpSampling2D()(concat1)\n",
    "    output = decoder_block(\n",
    "        output, 'decoder_output', num_filters=32)\n",
    "    output = Conv2D(\n",
    "        1, (1, 1), activation=None, name='prediction')(output)\n",
    "    if not use_lovash:\n",
    "        output = Activation('sigmoid')(output)\n",
    "    \n",
    "    \"\"\"\n",
    "    モデルのアセンブル \n",
    "    \"\"\"\n",
    "    model = Model(base_model.input, output)\n",
    "    model.compile(loss=loss_func, optimizer='adam', metrics=metrics_list)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 【問題2】コードの書き換え\n",
    "エンコーダーにResNetが使用されていたコードをVGGに変更してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def unet_resnet(input_size, decoder_block,\n",
    "                weights='imagenet',\n",
    "                loss_func='binary_crossentropy',\n",
    "                metrics_list=[my_iou_metric],\n",
    "                use_lovash=False):\n",
    "    \n",
    "    inputs = Input(shape=input_size)\n",
    "    \n",
    "    \"\"\"\n",
    "    エンコーダとしてVGG19を設定\n",
    "    weights='imagenet'でImageNetの学習済み重みを設定\n",
    "    \"\"\"\n",
    "    base_model = VGG19(\n",
    "       weights=weights,\n",
    "       include_top=False,\n",
    "       input_shape=input_size)\n",
    "    \n",
    "    #VGG19層凍結設定\n",
    "    for layer in base_model.layers:\n",
    "      layer.trainable = True\n",
    "    \n",
    "    \"\"\"\n",
    "    対応するデコーダの層のサイズからVGGどの層の出力をContructing pathとするか決める\n",
    "    \"\"\"\n",
    "    # Layers for feature extraction in the encoder part\n",
    "    encoder1 = base_model.get_layer('block1_pool').output\n",
    "    encoder2 = base_model.get_layer('block3_conv4').output\n",
    "    encoder3 = base_model.get_layer('block4_conv4').output\n",
    "    encoder4 = base_model.get_layer('block5_conv4').output\n",
    "    encoder5 = base_model.get_layer('block5_pool').output\n",
    "    #Center block\n",
    "    center = decoder_block(\n",
    "        encoder5, 'center', num_filters=512)\n",
    "        \n",
    "    concat5 = concatenate([center, encoder5], axis=-1)\n",
    "    # Decoder part.\n",
    "    # Every decoder block processed concatenated output from encoder and decoder part.\n",
    "    # This creates skip connections.\n",
    "    # Afterwards, decoder output is upsampled to dimensions equal to encoder output part.\n",
    "    decoder4 = decoder_block(\n",
    "        concat5, 'decoder4', num_filters=256)\n",
    "    concat4 = concatenate([UpSampling2D()(decoder4), encoder4], axis=-1)\n",
    "\n",
    "    decoder3 = decoder_block(\n",
    "        concat4, 'decoder3', num_filters=128)\n",
    "    concat3 = concatenate([UpSampling2D()(decoder3), encoder3], axis=-1)\n",
    "\n",
    "    decoder2 = decoder_block(\n",
    "        concat3, 'decoder2', num_filters=64)\n",
    "    concat2 = concatenate([UpSampling2D()(decoder2), encoder2], axis=-1)\n",
    "\n",
    "    decoder1 = decoder_block(\n",
    "        concat2, 'decoder1', num_filters=64)\n",
    "    concat1 = concatenate([UpSampling2D()(decoder1), encoder1], axis=-1)\n",
    "\n",
    "    # Final upsampling and decoder block for segmentation.\n",
    "    output = UpSampling2D()(concat1)\n",
    "    output = decoder_block(\n",
    "        output, 'decoder_output', num_filters=32)\n",
    "    output = Conv2D(\n",
    "        1, (1, 1), activation=None, name='prediction')(output)\n",
    "    if not use_lovash:\n",
    "        output = Activation('sigmoid')(output) \n",
    "    \n",
    "    model = Model(base_model.inputs, output)\n",
    "    model.compile(loss=loss_func, optimizer='adam', metrics=metrics_list)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 【問題3】学習・推定\n",
    "ResNetとVGG双方のコードで学習・推定を行い、結果を比較してください。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Epoch=40, batch=16で学習"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### ResNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"ResNet/table.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"ResNet/graph.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### VGG (w/o training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"VGG19_wo_training/table.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<img src=\"VGG19_wo_training/graph.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### VGG (w training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"VGG19_w_training/table.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<img src=\"VGG19_w_training/graph.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "結果、ResNetが最も良い結果となった。VGG19を用いた転移学習では層の凍結をしないほうが良い結果となった。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
