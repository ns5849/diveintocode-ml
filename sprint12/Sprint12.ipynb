{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class GetMiniBatch:\n",
    "    \"\"\"\n",
    "    ミニバッチを取得するイテレータ\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : 次の形のndarray, shape (n_samples, n_features)\n",
    "      学習データ\n",
    "    y : 次の形のndarray, shape (n_samples, 1)\n",
    "      正解値\n",
    "    batch_size : int\n",
    "      バッチサイズ\n",
    "    seed : int\n",
    "      NumPyの乱数のシード\n",
    "    \"\"\"\n",
    "    def __init__(self, X, y, batch_size = 10, seed=0):\n",
    "        self.batch_size = batch_size\n",
    "        np.random.seed(seed)\n",
    "        shuffle_index = np.random.permutation(np.arange(X.shape[0]))\n",
    "        self.X = X[shuffle_index]\n",
    "        self.y = y[shuffle_index]\n",
    "        self._stop = np.ceil(X.shape[0]/self.batch_size).astype(np.int)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self._stop\n",
    "\n",
    "    def __getitem__(self,item):\n",
    "        p0 = item*self.batch_size\n",
    "        p1 = item*self.batch_size + self.batch_size\n",
    "        return self.X[p0:p1], self.y[p0:p1]        \n",
    "\n",
    "    def __iter__(self):\n",
    "        self._counter = 0\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        if self._counter >= self._stop:\n",
    "            raise StopIteration()\n",
    "        p0 = self._counter*self.batch_size\n",
    "        p1 = self._counter*self.batch_size + self.batch_size\n",
    "        self._counter += 1\n",
    "        return self.X[p0:p1], self.y[p0:p1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 553,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class FC:\n",
    "    \"\"\"\n",
    "    ノード数n_nodes1からn_nodes2への全結合層\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_nodes1 : int\n",
    "      前の層のノード数\n",
    "    n_nodes2 : int\n",
    "      後の層のノード数\n",
    "    initializer : 初期化方法のインスタンス\n",
    "    optimizer : 最適化手法のインスタンス\n",
    "    \"\"\"\n",
    "    def __init__(self, n_nodes1, n_nodes2, initializer, optimizer, dropout_rate=0.5):\n",
    "        self.optimizer = optimizer\n",
    "        # 初期化\n",
    "        # initializerのメソッドを使い、self.Wとself.Bを初期化する\n",
    "        self.W = initializer.W(n_nodes1, n_nodes2)\n",
    "        self.B = initializer.B(n_nodes2)\n",
    "        self.W_feedback = 0\n",
    "        self.B_feedback = 0\n",
    "        self.dZ = 0\n",
    "        self.dA = 0\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.mask = None\n",
    "        self.input_X_forward = 0\n",
    "        \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        フォワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (batch_size, n_nodes1)\n",
    "            入力\n",
    "        Returns\n",
    "        ----------\n",
    "        A : 次の形のndarray, shape (batch_size, n_nodes2)\n",
    "            出力\n",
    "        \"\"\" \n",
    "        self.input_X_forward = X\n",
    "        A = np.dot(X, self.W) + self.B\n",
    "        \n",
    "        return A\n",
    "    \n",
    "    def backward(self, dA):\n",
    "        \"\"\"\n",
    "        バックワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        dA : 次の形のndarray, shape (batch_size, n_nodes2)\n",
    "            後ろから流れてきた勾配\n",
    "        Returns\n",
    "        ----------\n",
    "        dZ : 次の形のndarray, shape (batch_size, n_nodes1)\n",
    "            前に流す勾配\n",
    "        \"\"\"\n",
    "        \n",
    "        #print(\"input_X_forward.shape:\",self.input_X_forward.shape)\n",
    "        #print(\"dA.shape:\",dA.shape)\n",
    "        \n",
    "        dW = np.dot(self.input_X_forward.T, dA)\n",
    "        dZ = np.dot(dA, self.W.T)\n",
    "        self.dA = dA\n",
    "        self.dW = dW\n",
    "        self.dZ = dZ\n",
    "        \n",
    "        self.W_feedback = self.dW / self.dA.shape[0]\n",
    "        self.B_feedback = np.average(self.dA, axis=0)\n",
    "        \n",
    "        # 更新\n",
    "        self = self.optimizer.update(self)\n",
    "        return dZ\n",
    "    \n",
    "    def dropout_forward(self, X, flag):\n",
    "        if flag:\n",
    "            self.mask = np.random.rand(*X.shape) > self.dropout_rate\n",
    "            return X * self.mask\n",
    "        else:\n",
    "            return X * (1.0 - self.dropout_rate)\n",
    "        \n",
    "    def dropout_backward(self, X): \n",
    "        return X * self.mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Sigmoid:\n",
    "    \"\"\"\n",
    "    活性化関数 : Sigmoid\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_nodes1 : int\n",
    "      前の層のノード数\n",
    "    n_nodes2 : int\n",
    "      後の層のノード数\n",
    "    initializer : 初期化方法のインスタンス\n",
    "    optimizer : 最適化手法のインスタンス\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        # 初期化\n",
    "        self.input_X_forward = 0\n",
    "    \n",
    "    def _func(self, X):\n",
    "        return 1 / (1 + np.exp(-1 * X))\n",
    "    \n",
    "    def _func_diff(self, X):\n",
    "        return (1 - self._func(X)) * self._func(X)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        フォワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (batch_size, n_nodes1)\n",
    "            入力\n",
    "        Returns\n",
    "        ----------\n",
    "        A : 次の形のndarray, shape (batch_size, n_nodes2)\n",
    "            出力\n",
    "        \"\"\" \n",
    "        self.input_X_forward = X\n",
    "        A = self._func(X)\n",
    "        return A\n",
    "    \n",
    "    def backward(self, dA):\n",
    "        \"\"\"\n",
    "        バックワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        dA : 次の形のndarray, shape (batch_size, n_nodes2)\n",
    "            後ろから流れてきた勾配\n",
    "        Returns\n",
    "        ----------\n",
    "        dZ : 次の形のndarray, shape (batch_size, n_nodes1)\n",
    "            前に流す勾配\n",
    "        \"\"\"\n",
    "        \n",
    "        grad = self._func_diff(self.input_X_forward)\n",
    "        dZ = grad * dA\n",
    "        return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Tanh:\n",
    "    \"\"\"\n",
    "    活性化関数 : Sigmoid\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_nodes1 : int\n",
    "      前の層のノード数\n",
    "    n_nodes2 : int\n",
    "      後の層のノード数\n",
    "    initializer : 初期化方法のインスタンス\n",
    "    optimizer : 最適化手法のインスタンス\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        # 初期化\n",
    "        self.input_X_forward = 0\n",
    "    \n",
    "    def _func(self, X):\n",
    "        return np.tanh(X)\n",
    "    \n",
    "    def _func_diff(self, X):\n",
    "        return 1 - (self._func(X))**2\n",
    "        \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        フォワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (batch_size, n_nodes1)\n",
    "            入力\n",
    "        Returns\n",
    "        ----------\n",
    "        A : 次の形のndarray, shape (batch_size, n_nodes2)\n",
    "            出力\n",
    "        \"\"\" \n",
    "        self.input_X_forward = X\n",
    "        A = self._func(X)\n",
    "        return A\n",
    "    \n",
    "    def backward(self, dA):\n",
    "        \"\"\"\n",
    "        バックワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        dA : 次の形のndarray, shape (batch_size, n_nodes2)\n",
    "            後ろから流れてきた勾配\n",
    "        Returns\n",
    "        ----------\n",
    "        dZ : 次の形のndarray, shape (batch_size, n_nodes1)\n",
    "            前に流す勾配\n",
    "        \"\"\"\n",
    "        \n",
    "        grad = self._func_diff(self.input_X_forward)\n",
    "        dZ = grad * dA\n",
    "        return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class softmax:\n",
    "    \"\"\"\n",
    "    活性化関数 : Sigmoid\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_nodes1 : int\n",
    "      前の層のノード数\n",
    "    n_nodes2 : int\n",
    "      後の層のノード数\n",
    "    initializer : 初期化方法のインスタンス\n",
    "    optimizer : 最適化手法のインスタンス\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        # 初期化\n",
    "        self.input_X_forward = 0\n",
    "        self.pred = 0\n",
    "    \n",
    "    def _func(self, X):\n",
    "        X = X - np.max(X)\n",
    "        tmp = np.exp(X)\n",
    "        denominator = np.sum(tmp, axis=1)\n",
    "        output = tmp / denominator[:, np.newaxis]\n",
    "        return output\n",
    "    \n",
    "    def _func_diff(self, X):\n",
    "        return X\n",
    "        \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        フォワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (batch_size, n_nodes1)\n",
    "            入力\n",
    "        Returns\n",
    "        ----------\n",
    "        A : 次の形のndarray, shape (batch_size, n_nodes2)\n",
    "            出力\n",
    "        \"\"\" \n",
    "        self.input_X_forward = X\n",
    "        A = self._func(X)\n",
    "        self.pred = A\n",
    "        return A\n",
    "    \n",
    "    def backward(self, dA):\n",
    "        \"\"\"\n",
    "        バックワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        dA : 次の形のndarray, shape (batch_size, n_nodes2)\n",
    "            後ろから流れてきた勾配\n",
    "        Returns\n",
    "        ----------\n",
    "        dZ : 次の形のndarray, shape (batch_size, n_nodes1)\n",
    "            前に流す勾配\n",
    "        \"\"\"\n",
    "        dZ = self.pred - dA\n",
    "        \n",
    "        return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SimpleInitializer:\n",
    "    \"\"\"\n",
    "    ガウス分布によるシンプルな初期化\n",
    "    Parameters\n",
    "    ----------\n",
    "    sigma : float\n",
    "      ガウス分布の標準偏差\n",
    "    \"\"\"\n",
    "    def __init__(self, sigma = 0.01):\n",
    "        self.sigma = sigma\n",
    "        \n",
    "    def W(self, n_nodes1, n_nodes2):\n",
    "        W = self.sigma * np.random.randn(n_nodes1, n_nodes2)\n",
    "        return W\n",
    "    \n",
    "    def B(self, n_nodes2):\n",
    "        B = self.sigma * np.random.randn(1, n_nodes2)\n",
    "        return B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ReLU:\n",
    "    \"\"\"\n",
    "    活性化関数 : Sigmoid\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_nodes1 : int\n",
    "      前の層のノード数\n",
    "    n_nodes2 : int\n",
    "      後の層のノード数\n",
    "    initializer : 初期化方法のインスタンス\n",
    "    optimizer : 最適化手法のインスタンス\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        # 初期化\n",
    "        self.input_X_forward = 0\n",
    "    \n",
    "    def _func(self, X):\n",
    "        return np.maximum(0, X)\n",
    "    \n",
    "    def _func_diff(self, X):\n",
    "        return np.where( x > 0, 1, 0)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        フォワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (batch_size, n_nodes1)\n",
    "            入力\n",
    "        Returns\n",
    "        ----------\n",
    "        A : 次の形のndarray, shape (batch_size, n_nodes2)\n",
    "            出力\n",
    "        \"\"\" \n",
    "        self.input_X_forward = X\n",
    "        A = self._func(X)\n",
    "        return A\n",
    "    \n",
    "    def backward(self, dA):\n",
    "        \"\"\"\n",
    "        バックワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        dA : 次の形のndarray, shape (batch_size, n_nodes2)\n",
    "            後ろから流れてきた勾配\n",
    "        Returns\n",
    "        ----------\n",
    "        dZ : 次の形のndarray, shape (batch_size, n_nodes1)\n",
    "            前に流す勾配\n",
    "        \"\"\"\n",
    "        \n",
    "        grad = self._func_diff(self.input_X_forward)\n",
    "        dZ = grad * dA\n",
    "        return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class XavierInitializer:\n",
    "    \"\"\"\n",
    "    Xavierによる初期化\n",
    "    Parameters\n",
    "    ----------\n",
    "    sigma : float\n",
    "      ガウス分布の標準偏差\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.n_prev_nodes = 1\n",
    "        pass\n",
    "        \n",
    "    def W(self, n_nodes1, n_nodes2):\n",
    "        self.n_prev_nodes = n_nodes1\n",
    "        W = np.random.randn(n_nodes1, n_nodes2) / np.sqrt(n_nodes1)\n",
    "        return W\n",
    "    \n",
    "    def B(self, n_nodes2):\n",
    "        B = np.random.randn(1, n_nodes2) / np.sqrt(self.n_prev_nodes)\n",
    "        return B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class HeInitializer:\n",
    "    \"\"\"\n",
    "    Heによる初期化\n",
    "    Parameters\n",
    "    ----------\n",
    "    sigma : float\n",
    "      ガウス分布の標準偏差\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.n_prev_nodes = 1\n",
    "        pass\n",
    "        \n",
    "    def W(self, n_nodes1, n_nodes2):\n",
    "        self.n_prev_nodes = n_nodes1\n",
    "        W = np.random.randn(n_nodes1, n_nodes2) * np.sqrt(2 / n_nodes1)\n",
    "        return W\n",
    "    \n",
    "    def B(self, n_nodes2):\n",
    "        B = np.random.randn(1, n_nodes2) * np.sqrt(2 / self.n_prev_nodes)\n",
    "        return B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class AdaGrad:\n",
    "    \"\"\"\n",
    "    確率的勾配降下法\n",
    "    Parameters\n",
    "    ----------\n",
    "    lr : 学習率\n",
    "    \"\"\"\n",
    "    def __init__(self, lr):\n",
    "        self.lr = lr\n",
    "        self.H_B = 1\n",
    "        self.H_W = 1\n",
    "    def update(self, layer):\n",
    "        \"\"\"\n",
    "        ある層の重みやバイアスの更新\n",
    "        Parameters\n",
    "        ----------\n",
    "        layer : 更新前の層のインスタンス\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        layer : 更新後の層のインスタンス\n",
    "        \"\"\"\n",
    "        \n",
    "        #dA, dWを更新＆保存\n",
    "        self.H_B = self.H_B + np.average(layer.dA)**2\n",
    "        self.H_W = self.H_W + np.average(layer.dW)**2\n",
    "        \n",
    "        layer.B = layer.B - self.lr * np.average(layer.dA, axis=0) / np.sqrt(self.H_B)\n",
    "        layer.W = layer.W - self.lr * layer.dW / layer.dA.shape[0] / np.sqrt(self.H_W)\n",
    "        \n",
    "        return layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SGD:\n",
    "    \"\"\"\n",
    "    確率的勾配降下法\n",
    "    Parameters\n",
    "    ----------\n",
    "    lr : 学習率\n",
    "    \"\"\"\n",
    "    def __init__(self, lr):\n",
    "        self.lr = lr\n",
    "    def update(self, layer):\n",
    "        \"\"\"\n",
    "        ある層の重みやバイアスの更新\n",
    "        Parameters\n",
    "        ----------\n",
    "        layer : 更新前の層のインスタンス\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        layer : 更新後の層のインスタンス\n",
    "        \"\"\"\n",
    "        layer.B = layer.B - self.lr * layer.B_feedback    \n",
    "        layer.W = layer.W - self.lr * layer.W_feedback\n",
    "        \n",
    "        return layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 【問題1】チャンネル数を1に限定した1次元畳み込み層クラスの作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class SimpleConv1d():\n",
    "    \"\"\"\n",
    "    ノード数n_nodes1からn_nodes2への全結合層\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_nodes1 : int\n",
    "      前の層のノード数\n",
    "    n_nodes2 : int\n",
    "      後の層のノード数\n",
    "    initializer : 初期化方法のインスタンス\n",
    "    optimizer : 最適化手法のインスタンス\n",
    "    \"\"\"\n",
    "    def __init__(self, n_input_hight, f_w, f_b, optimizer):\n",
    "        DIM = 1\n",
    "        \n",
    "        self.optimizer = optimizer\n",
    "        # 初期化\n",
    "        # initializerのメソッドを使い、self.Wとself.Bを初期化する\n",
    "        self.f_hight = len(f_w)\n",
    "        self.n_input_hight = n_input_hight\n",
    "        #self.n_input_width = n_input_width\n",
    "        self.W = f_w[:, np.newaxis]\n",
    "        self.B = f_b[:, np.newaxis]\n",
    "        self.dZ = 0\n",
    "        self.dA = 0\n",
    "        self.dB = 0\n",
    "        print(\"N_input:{} F_hight:{}\".format(self.n_input_hight, self.f_hight))\n",
    "        self.n_output_hight = self.n_input_hight - self.f_hight + 1\n",
    "        #self.n_output_width = self.n_input_width - f_width +1\n",
    "        self.input_X_forward = 0\n",
    "        self.output_X_forward = np.zeros([self.n_output_hight, DIM])\n",
    "        self.W_feedback = np.zeros([self.f_hight, DIM])\n",
    "        self.B_feedback = 0\n",
    "        self.Z_feedback = np.zeros([self.n_input_hight, DIM])\n",
    "        \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        フォワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (batch_size, n_nodes1)\n",
    "            入力\n",
    "        Returns\n",
    "        ----------\n",
    "        A : 次の形のndarray, shape (batch_size, n_nodes2)\n",
    "            出力\n",
    "        \"\"\" \n",
    "        \n",
    "        self.input_X_forward = X\n",
    "        for h in range(self.n_output_hight):\n",
    "            h1 = h\n",
    "            h2 = h + self.f_hight\n",
    "            \n",
    "            X_seg = X[h1:h2]\n",
    "            self.output_X_forward[h] = np.dot(X_seg, self.W) + self.B\n",
    "        \n",
    "        return self.output_X_forward\n",
    "    \n",
    "    def backward(self, dA):\n",
    "        \"\"\"\n",
    "        バックワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        dA : 次の形のndarray, shape (batch_size, n_nodes2)\n",
    "            後ろから流れてきた勾配\n",
    "        Returns\n",
    "        ----------\n",
    "        dZ : 次の形のndarray, shape (batch_size, n_nodes1)\n",
    "            前に流す勾配\n",
    "        \"\"\"\n",
    "        \n",
    "        #Wについて\n",
    "        dA = dA[:,np.newaxis]\n",
    "        for i in range(self.f_hight):\n",
    "            X_seg = self.input_X_forward[i : (i + self.n_output_hight)]\n",
    "            X_seg = X_seg[:,np.newaxis]\n",
    "            self.W_feedback[i] = np.dot(X_seg.T, dA)\n",
    "        \n",
    "        #Bについて\n",
    "        self.B_feedback = np.sum(dA, axis=0)\n",
    "        \n",
    "        #Zについて\n",
    "        #損失(行列)の端の処理のため、列の前後に0列を追加（フィルタサイズから計算）\n",
    "        dA_padding = np.zeros([self.f_hight-1, 1])\n",
    "        dA = np.concatenate((dA, dA_padding), axis=0)\n",
    "        dA = np.concatenate((dA_padding, dA), axis=0)\n",
    "        for h in range(self.n_input_hight):\n",
    "            h1 = h\n",
    "            h2 = h + self.f_hight\n",
    "            dA_seg = dA[h1:h2]\n",
    "            #並列計算工夫\n",
    "            dA_seg = np.fliplr(dA_seg.T).T\n",
    "            self.Z_feedback[h] = np.dot(dA_seg.T, self.W)\n",
    "            #print(\"h:{} \\n dA_seg:{} \\n W:{}\".format(h, dA_seg, self.W))\n",
    "\n",
    "        # 更新\n",
    "        self = self.optimizer.update(self)\n",
    "        return self.Z_feedback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 【問題2】1次元畳み込み後の出力サイズの計算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cal_output_size(n_feature_in, n_pading, n_filter, stride):\n",
    "    return (n_feature_in + n_pading * 2 + n_filter) / stride"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 【問題3】小さな配列での1次元畳み込み層の実験"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = np.array([1,2,3,4])\n",
    "w = np.array([3, 5, 7])\n",
    "b = np.array([1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#initializer = SimpleInitializer()\n",
    "optimizer = SGD(0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N_input:4 F_hight:3\n"
     ]
    }
   ],
   "source": [
    "scv = SimpleConv1d(len(x), w, b, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[35.],\n",
       "       [50.]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scv.forward(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "delta_a = np.array([10, 20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 30.],\n",
       "       [110.],\n",
       "       [170.],\n",
       "       [140.]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scv.backward(delta_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "delta_b: [30]\n",
      "delta_w: [[ 50.]\n",
      " [ 80.]\n",
      " [110.]]\n",
      "delta_x: [[ 30.]\n",
      " [110.]\n",
      " [170.]\n",
      " [140.]]\n"
     ]
    }
   ],
   "source": [
    "print(\"delta_b:\",scv.B_feedback)\n",
    "print(\"delta_w:\",scv.W_feedback)\n",
    "print(\"delta_x:\",scv.Z_feedback)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "##### 【問題4】チャンネル数を限定しない1次元畳み込み層クラスの作成\n",
    "チャンネル数を1に限定しない1次元畳み込み層のクラスConv1dを作成してください。\n",
    "紙やホワイトボードを使い計算グラフを書きながら考えてください。\n",
    "例えば以下のようなx, w, bがあった場合は"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "x = np.array([[1, 2, 3, 4], [2, 3, 4, 5]]) # shape(2, 4)で、（入力チャンネル数、特徴量数）である。  \n",
    "w = np.ones((3, 2, 3)) # 例の簡略化のため全て1とする。(出力チャンネル数、入力チャンネル数、フィルタサイズ)である。  \n",
    "b = np.array([1, 2, 3]) # （出力チャンネル数）  \n",
    "a = np.array([[16, 22], [17, 23], [18, 24]]) # shape(3, 2)で、（出力チャンネル数、特徴量数）である。  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Conv1d():\n",
    "    \"\"\"\n",
    "    ノード数n_nodes1からn_nodes2への全結合層\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_nodes1 : int\n",
    "      前の層のノード数\n",
    "    n_nodes2 : int\n",
    "      後の層のノード数\n",
    "    initializer : 初期化方法のインスタンス\n",
    "    optimizer : 最適化手法のインスタンス\n",
    "    \"\"\"\n",
    "    def __init__(self, n_input_hight, f_w, f_b, initializer, optimizer):\n",
    "        self.optimizer = optimizer\n",
    "        # 初期化\n",
    "        # initializerのメソッドを使い、self.Wとself.Bを初期化する\n",
    "        self.f_hight = f_w.shape[2]\n",
    "        self.n_input_hight = n_input_hight\n",
    "        #self.n_input_width = n_input_width\n",
    "        self.W = f_w\n",
    "        self.B = f_b\n",
    "        self.n_output_hight = self.n_input_hight - self.f_hight + 1\n",
    "        self.input_X_forward = 0\n",
    "        self.output_X_forward = np.zeros((self.W.shape[0], self.n_output_hight))\n",
    "        self.W_feedback = np.zeros_like(self.W)\n",
    "        self.B_feedback = np.zeros_like(self.B)\n",
    "        self.Z_feedback = 0\n",
    "    \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        フォワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (batch_size, n_nodes1)\n",
    "            入力\n",
    "        Returns\n",
    "        ----------\n",
    "        A : 次の形のndarray, shape (batch_size, n_nodes2)\n",
    "            出力\n",
    "        \"\"\" \n",
    "        \n",
    "        self.input_X_forward = X\n",
    "        #for output_ch in range(self.W.shape[0]):\n",
    "        A = np.zeros((self.n_output_hight, self.W.shape[0]))\n",
    "        #self.input_X_forward = np.zeros((self.n_output_hight, X.shape[0], self.f_hight))\n",
    "        for h in range(self.n_output_hight):\n",
    "            h1 = h\n",
    "            h2 = h + self.f_hight   \n",
    "            X_seg = X[:, h1:h2]\n",
    "            tmp = np.sum(X_seg * self.W, axis=1)\n",
    "            A[h] = np.sum(tmp, axis=1)\n",
    "\n",
    "        B = self.B[0]\n",
    "        output = (A + B).T\n",
    "        \n",
    "        #print(\"output.shape:\",output.shape)\n",
    "        return output\n",
    "    \n",
    "    def backward(self, dA):\n",
    "        \"\"\"\n",
    "        バックワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        dA : 次の形のndarray, shape (batch_size, n_nodes2)\n",
    "            後ろから流れてきた勾配\n",
    "        Returns\n",
    "        ----------\n",
    "        dZ : 次の形のndarray, shape (batch_size, n_nodes1)\n",
    "            前に流す勾配\n",
    "        \"\"\"\n",
    "        \n",
    "        #Wについて\n",
    "        #Row = 入力数＊出力数, Col = 出力の特徴量数　の入力X,誤差dLを作る \n",
    "        X = np.tile(self.input_X_forward, (dA.shape[0] ,1))\n",
    "        dL = np.zeros((X.shape[0], dA.shape[1]))\n",
    "        \n",
    "        for i in range(dA.shape[0]):\n",
    "            o1 = i\n",
    "            o2 = i + self.input_X_forward.shape[0]\n",
    "            dL[o1:o2] = np.tile(dA[i], (self.input_X_forward.shape[0] ,1))\n",
    "        \n",
    "        #入力の特徴量数 - 出力の特徴量数 +1\n",
    "        loop = self.input_X_forward.shape[1] - dA.shape[1] + 1\n",
    "        dW_tmp = np.zeros((X.shape[0], loop))\n",
    "        for i in range(loop):\n",
    "            i1 = i\n",
    "            i2 = i + dA.shape[1]\n",
    "            dX_seg = X[:, i1:i2]\n",
    "            dW_tmp[:,i] = np.sum(dL * dX_seg, axis=1)\n",
    "        \n",
    "        #計算結果をフィルタサイズに整形\n",
    "        for i in range(self.W.shape[0]):\n",
    "            o1 = i\n",
    "            o2 = i + self.W.shape[1]\n",
    "            self.W_feedback[i] = dW_tmp[o1:o2]\n",
    "        \n",
    "        #Bについて\n",
    "        dB = np.sum(dA, axis=1)\n",
    "        for i in range(self.B.shape[1]):\n",
    "            self.B_feedback[:,i] = dB\n",
    "        \n",
    "        #Zについて Output数回す\n",
    "        self.Z_feedback = np.zeros_like(self.input_X_forward)\n",
    "        for i in range(dA.shape[0]):\n",
    "            #損失(行列)の端の処理のため、列の前後に0列を追加（フィルタサイズから計算）\n",
    "            dA_padding = np.zeros([1, self.f_hight-1])\n",
    "            dA_tmp = dA[i][np.newaxis,:]\n",
    "            dA_tmp = np.concatenate((dA_tmp, dA_padding), axis=1)\n",
    "            dA_tmp = np.concatenate((dA_padding, dA_tmp), axis=1)\n",
    "            dA_tmp = np.tile(dA_tmp, (self.input_X_forward.shape[0] ,1))\n",
    "            dZ_seg = np.zeros_like(self.Z_feedback)\n",
    "            \n",
    "            for h in range(self.n_input_hight):\n",
    "                h1 = h\n",
    "                h2 = h + self.f_hight\n",
    "                dA_seg = dA_tmp[:,h1:h2]\n",
    "                #並列計算工夫\n",
    "                dA_seg = np.fliplr(dA_seg.T).T\n",
    "                dZ_seg[:,h] = np.sum(dA_seg * self.W[i], axis=1)\n",
    "                \n",
    "            self.Z_feedback += dZ_seg #出力数分足し算\n",
    "\n",
    "        # 更新\n",
    "        self = self.optimizer.update(self)\n",
    "        return self.Z_feedback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = np.array([[1,2,3,4],[2,3,4,5]])\n",
    "W = np.ones((3,2,3))\n",
    "B = np.array([[[1,2,3], [1,2,3]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "c = Conv1d(X.shape[1], W, B, None, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[16., 22.],\n",
       "       [17., 23.],\n",
       "       [18., 24.]])"
      ]
     },
     "execution_count": 341,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.forward(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dA = np.array([[2,4],[4,8],[6,12]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[12, 36, 36, 24],\n",
       "       [12, 36, 36, 24]])"
      ]
     },
     "execution_count": 343,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.backward(dA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 【問題5】学習・推定\n",
    "これまで使ってきたニューラルネットワークの全結合層の一部をConv1dに置き換えて学習と推定を行ってください。出力層だけは全結合層をそのまま使ってください。\n",
    "チャンネルが複数ある状態では全結合層への入力は行えません。その段階でのチャンネルは1になるようにするか、平滑化を行います。平滑化はNumPyのreshapeが使用できます。\n",
    "numpy.reshape — NumPy v1.15 Manual\n",
    "画像に対しての1次元畳み込みは実用上は行わないことのため、精度は問いません。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "default_dnn_design = {\n",
    "    'learning_rate':0.001,\n",
    "    'total_layer':3,\n",
    "    'func_layer1':'tanh',\n",
    "    'func_layer2':'tanh',\n",
    "    'func_layer3':'softmax',\n",
    "    'node_layer0':786, \n",
    "    'node_layer1':400,\n",
    "    'node_layer2':200,\n",
    "    'node_layer3':10,\n",
    "    'initializer':'SimpleInitializer',\n",
    "    'initializer_sigma':0.05,\n",
    "    'optimizer':'SGD',\n",
    "}\n",
    "\n",
    "class ScratchDeepNeuralNetrowkClassifier():\n",
    "    \"\"\"\n",
    "    ディープニューラルネットワーク分類器\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_epoch, batch_size, verbose = False):\n",
    "        self.verbose = verbose\n",
    "        self.batch_size = batch_size\n",
    "        self.n_epoch = n_epoch\n",
    "        self.loss = 0\n",
    "        self.loss_val = 0\n",
    "        self.activation_func = 0\n",
    "        self.affine_func = 0\n",
    "        self.n_layer = 0\n",
    "        self.layer_instance = [0 for _ in range(64)]\n",
    "        #self.activation_func = [0 for _ in range(self.dnn_design.get('total_layer'))]\n",
    "        #self.affine_func = [0 for _ in range(self.dnn_design.get('total_layer'))]\n",
    "        #self.n_layer = self.dnn_design.get('total_layer')\n",
    "        \n",
    "        #各インスタンスを生成\n",
    "        #initializerインスタンス\n",
    "        \n",
    "    def _crossentropy(self, y_pred, y):\n",
    "        #クロスエントロピーを計算する\n",
    "        INF_AVOIDANCE = 1e-8\n",
    "        cross_entropy = -1 * y * np.log(y_pred + INF_AVOIDANCE)\n",
    "        return np.sum(cross_entropy, axis=1)\n",
    "    \n",
    "    def add_layer(self, model):\n",
    "        self.layer_instance[self.n_layer] = model\n",
    "        self.n_layer += 1\n",
    "        return\n",
    "    \n",
    "    def delet_all_layer(self):\n",
    "        #add_layerでセットしたlayer情報を全てクリアする\n",
    "        self.layer_instance[0:self.n_layer] = 0\n",
    "        self.n_layer = 0\n",
    "        \n",
    "        return\n",
    "    \n",
    "    def fit(self, X, y, X_val=None, y_val=None):\n",
    "        #lossの記録用の配列を用意\n",
    "        self.loss = [[0 for i in range(X.shape[0])] for j in range(self.n_epoch)]\n",
    "        self.loss_val = [[0 for i in range(X.shape[0])] for j in range(self.n_epoch)]\n",
    "        \n",
    "        i = 0\n",
    "        get_mini_batch = GetMiniBatch(x_train, y_train, self.batch_size)\n",
    "        for epoch in range(self.n_epoch):\n",
    "            loop_count = 0\n",
    "            sum_loss = 0\n",
    "            for mini_X_train, mini_y_train in get_mini_batch:\n",
    "                X = mini_X_train\n",
    "                #Forwardの計算\n",
    "                for layer in range(self.n_layer):\n",
    "                    #X = self.affine_func[layer].forward(X)\n",
    "                    #X = self.activation_func[layer].forward(X)\n",
    "                    X = self.layer_instance[layer].forward(X)\n",
    "                \n",
    "                #Loss計算\n",
    "                sum_loss += self._crossentropy(X, mini_y_train)\n",
    "                    \n",
    "                #Backwardの計算\n",
    "                dz = mini_y_train\n",
    "                for layer in reversed(range(0, self.n_layer)):\n",
    "                    #dz = self.activation_func[layer].backward(dz)\n",
    "                    #dz = self.affine_func[layer].backward(dz)\n",
    "                    dz = self.layer_instance[layer].backward(dz)\n",
    "                \n",
    "                loop_count += 1\n",
    "                \n",
    "            #Epoch毎のLoss計算結果表示\n",
    "            self.loss[i] = sum_loss / loop_count\n",
    "            if X_val is not None and y_val is not None:\n",
    "                y_val_pred = self._predict(X_val)\n",
    "                self.loss_val[i] = self._crossentropy(y_val_pred, y_val)\n",
    "                \n",
    "            if self.verbose:\n",
    "                #verboseをTrueにした際は学習過程などを出力する\n",
    "                print(\"Epoch:{} Loss:{} Loss(val):{}\".format(i, self.loss[i], self.loss_val[i]))\n",
    "                \n",
    "            i +=1\n",
    "            \n",
    "        return\n",
    "    \n",
    "    def predict(self, X):\n",
    "        #Forwardの計算\n",
    "        for layer in range(self.n_layer):\n",
    "            #X = self.affine_func[layer].forward(X)\n",
    "            #X = self.activation_func[layer].forward(X)\n",
    "            X = self.layer_instance[layer].forward(X)\n",
    "        \n",
    "        max_val = np.max(X, axis=1)\n",
    "        mask = np.ones_like(X)\n",
    "        X[X == max_val[:,np.newaxis]] = 1\n",
    "        X[X != mask] = 0        \n",
    "        \n",
    "        return X\n",
    "\n",
    "    def _predict(self, X):\n",
    "        #Forwardの計算\n",
    "        for layer in range(self.n_layer):\n",
    "            #X = self.affine_func[layer].forward(X)\n",
    "            #X = self.activation_func[layer].forward(X)  \n",
    "            X = self.layer_instance[layer].forward(X)\n",
    "        \n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Flatten():\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.input_X_shape = 0\n",
    "        \n",
    "    def forward(self, X):\n",
    "        self.inout_X_shape = X.shape\n",
    "        return X.reshape(-1)[np.newaxis,:]\n",
    "    \n",
    "    def backward(self, X):\n",
    "        output = X.reshape(self.inout_X_shape)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3000, 784)\n",
      "(57000, 784)\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import mnist\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "x_train = x_train.reshape(-1, 784)\n",
    "x_test = x_test.reshape(-1, 784)\n",
    "\n",
    "x_train = x_train.astype(np.float)\n",
    "x_test = x_test.astype(np.float)\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "\n",
    "enc = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "y_train_one_hot = enc.fit_transform(y_train[:, np.newaxis])\n",
    "y_test_one_hot = enc.transform(y_test[:, np.newaxis])\n",
    "\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train_one_hot, test_size=0.95)\n",
    "print(x_train.shape) # (48000, 784)\n",
    "print(x_val.shape) # (12000, 784)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "CNN = ScratchDeepNeuralNetrowkClassifier(1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#CNN Filter(重み、バイアス)\n",
    "f_w = np.ones((3,1,28))\n",
    "f_b = np.array([[[1,1,1]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#初期化、更新インスタンスを作る\n",
    "optimizer = SGD(0.01)\n",
    "initializer = XavierInitializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#DNNデザイン\n",
    "CNN.add_layer(Conv1d(x_train.shape[1], f_w, f_b, initializer, optimizer))\n",
    "CNN.add_layer(Flatten())\n",
    "CNN.add_layer(FC(f_w.shape[0] * (x_train.shape[1] - f_w.shape[2] + 1), 100, initializer, optimizer))\n",
    "CNN.add_layer(Sigmoid())\n",
    "CNN.add_layer(FC(100, 10, initializer, optimizer))\n",
    "CNN.add_layer(softmax())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/naoki/.pyenv/versions/anaconda3-4.3.0/lib/python3.6/site-packages/ipykernel/__main__.py:18: RuntimeWarning: overflow encountered in exp\n"
     ]
    }
   ],
   "source": [
    "CNN.fit(x_train, y_train, x_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/naoki/.pyenv/versions/anaconda3-4.3.0/lib/python3.6/site-packages/ipykernel/__main__.py:18: RuntimeWarning: overflow encountered in exp\n"
     ]
    }
   ],
   "source": [
    "y_pred = CNN.predict(x_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pred=\n",
      " [[0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]]\n",
      "Yval=\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "print(\"Pred=\\n\", y_pred)\n",
    "print(\"Yval=\\n\", y_val[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score=0.800\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "print(\"Accuracy score={:.3f}\".format(accuracy_score(y_pred[0], y_val[2])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 【問題6】（アドバンス課題）パディングの実装\n",
    "畳み込み層にパディングを加えてください。1次元配列の場合、前後にn個特徴量を増やせるようにしてください。\n",
    "最も単純なパディングは全て0で埋めるゼロパディングであり、CNNでは一般的です。他に端の値を繰り返す方法などもあります。\n",
    "フレームワークによっては、元の入力のサイズを保つようにという指定をすることができます。この機能も持たせておくと便利です。\n",
    "なお、NumPyにはパディングの関数が存在します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Conv1d():\n",
    "    \"\"\"\n",
    "    ノード数n_nodes1からn_nodes2への全結合層\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_nodes1 : int\n",
    "      前の層のノード数\n",
    "    n_nodes2 : int\n",
    "      後の層のノード数\n",
    "    initializer : 初期化方法のインスタンス\n",
    "    optimizer : 最適化手法のインスタンス\n",
    "    \"\"\"\n",
    "    def __init__(self, n_input_hight, padding_size, f_w, f_b, initializer, optimizer):\n",
    "        self.optimizer = optimizer\n",
    "        # 初期化\n",
    "        # initializerのメソッドを使い、self.Wとself.Bを初期化する\n",
    "        self.f_hight = f_w.shape[2]\n",
    "        self.padding_size = padding_size\n",
    "        self.n_input_hight = n_input_hight + self.padding_size * 2\n",
    "        #self.n_input_width = n_input_width\n",
    "        self.W = f_w\n",
    "        self.B = f_b\n",
    "        self.n_output_hight = self.n_input_hight - self.f_hight + 1\n",
    "        self.input_X_forward = 0\n",
    "        self.output_X_forward = np.zeros((self.W.shape[0], self.n_output_hight))\n",
    "        self.W_feedback = np.zeros_like(self.W)\n",
    "        self.B_feedback = np.zeros_like(self.B)\n",
    "        self.Z_feedback = 0\n",
    "    \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        フォワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (batch_size, n_nodes1)\n",
    "            入力\n",
    "        Returns\n",
    "        ----------\n",
    "        A : 次の形のndarray, shape (batch_size, n_nodes2)\n",
    "            出力\n",
    "        \"\"\" \n",
    "        \n",
    "        #Padding\n",
    "        X = np.pad(X, (self.padding_size, self.padding_size), 'constant', constant_values=(0, 0))\n",
    "        \n",
    "        self.input_X_forward = X\n",
    "        A = np.zeros((self.n_output_hight, self.W.shape[0]))\n",
    "        for h in range(self.n_output_hight):\n",
    "            h1 = h\n",
    "            h2 = h + self.f_hight   \n",
    "            X_seg = X[:, h1:h2]\n",
    "            tmp = np.sum(X_seg * self.W, axis=1)\n",
    "            A[h] = np.sum(tmp, axis=1)\n",
    "\n",
    "        B = self.B[0]\n",
    "        output = (A + B).T\n",
    "        \n",
    "        #print(\"output.shape:\",output.shape)\n",
    "        return output\n",
    "    \n",
    "    def backward(self, dA):\n",
    "        \"\"\"\n",
    "        バックワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        dA : 次の形のndarray, shape (batch_size, n_nodes2)\n",
    "            後ろから流れてきた勾配\n",
    "        Returns\n",
    "        ----------\n",
    "        dZ : 次の形のndarray, shape (batch_size, n_nodes1)\n",
    "            前に流す勾配\n",
    "        \"\"\"\n",
    "        \n",
    "        #Wについて\n",
    "        #Row = 入力数＊出力数, Col = 出力の特徴量数　の入力X,誤差dLを作る \n",
    "        X = np.tile(self.input_X_forward, (dA.shape[0] ,1))\n",
    "        dL = np.zeros((X.shape[0], dA.shape[1]))\n",
    "        \n",
    "        for i in range(dA.shape[0]):\n",
    "            o1 = i\n",
    "            o2 = i + self.input_X_forward.shape[0]\n",
    "            dL[o1:o2] = np.tile(dA[i], (self.input_X_forward.shape[0] ,1))\n",
    "        \n",
    "        #入力の特徴量数 - 出力の特徴量数 +1\n",
    "        loop = self.input_X_forward.shape[1] - dA.shape[1] + 1\n",
    "        dW_tmp = np.zeros((X.shape[0], loop))\n",
    "        for i in range(loop):\n",
    "            i1 = i\n",
    "            i2 = i + dA.shape[1]\n",
    "            dX_seg = X[:, i1:i2]\n",
    "            dW_tmp[:,i] = np.sum(dL * dX_seg, axis=1)\n",
    "        \n",
    "        #計算結果をフィルタサイズに整形\n",
    "        for i in range(self.W.shape[0]):\n",
    "            o1 = i\n",
    "            o2 = i + self.W.shape[1]\n",
    "            self.W_feedback[i] = dW_tmp[o1:o2]\n",
    "        \n",
    "        #Bについて\n",
    "        dB = np.sum(dA, axis=1)\n",
    "        for i in range(self.B.shape[1]):\n",
    "            self.B_feedback[:,i] = dB\n",
    "        \n",
    "        #Zについて Output数回す\n",
    "        self.Z_feedback = np.zeros_like(self.input_X_forward)\n",
    "        for i in range(dA.shape[0]):\n",
    "            #損失(行列)の端の処理のため、列の前後に0列を追加（フィルタサイズから計算）\n",
    "            dA_padding = np.zeros([1, self.f_hight-1])\n",
    "            dA_tmp = dA[i][np.newaxis,:]\n",
    "            dA_tmp = np.concatenate((dA_tmp, dA_padding), axis=1)\n",
    "            dA_tmp = np.concatenate((dA_padding, dA_tmp), axis=1)\n",
    "            dA_tmp = np.tile(dA_tmp, (self.input_X_forward.shape[0] ,1))\n",
    "            dZ_seg = np.zeros_like(self.Z_feedback)\n",
    "            \n",
    "            for h in range(self.n_input_hight):\n",
    "                h1 = h\n",
    "                h2 = h + self.f_hight\n",
    "                dA_seg = dA_tmp[:,h1:h2]\n",
    "                #並列計算工夫\n",
    "                dA_seg = np.fliplr(dA_seg.T).T\n",
    "                dZ_seg[:,h] = np.sum(dA_seg * self.W[i], axis=1)\n",
    "                \n",
    "            self.Z_feedback += dZ_seg #出力数分足し算\n",
    "\n",
    "        # 更新\n",
    "        self = self.optimizer.update(self)\n",
    "        return self.Z_feedback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 【問題7】（アドバンス課題）ミニバッチへの対応\n",
    "ここまでの課題はバッチサイズ1で良いとしてきました。しかし、実際は全結合層同様にミニバッチ学習が行われます。Conv1dクラスを複数のデータが同時に計算できるように変更してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3000, 1, 784)\n",
      "(57000, 1, 784)\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import mnist\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "x_train = x_train.reshape(-1, 784)\n",
    "x_test = x_test.reshape(-1, 784)\n",
    "\n",
    "#0~255を0~1スケールへ\n",
    "x_train = x_train.astype(np.float)\n",
    "x_test = x_test.astype(np.float)\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "\n",
    "#0~9をone-hot encoding\n",
    "enc = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "y_train_one_hot = enc.fit_transform(y_train[:, np.newaxis])\n",
    "y_test_one_hot = enc.transform(y_test[:, np.newaxis])\n",
    "\n",
    "#chを追加\n",
    "x_train = x_train[:,np.newaxis,:]\n",
    "x_test = x_test[:,np.newaxis,:]\n",
    "\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train_one_hot, test_size=0.95)\n",
    "print(x_train.shape) # (48000, 784)\n",
    "print(x_val.shape) # (12000, 784)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "get_mini_batch = GetMiniBatch(x_train, y_train, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mini_X_train, mini_y_train = get_mini_batch[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 1, 784)"
      ]
     },
     "execution_count": 458,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mini_X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Flatten():\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.input_X_shape = 0\n",
    "        \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        X.shape (batch_size, n_input, n_feature1)\n",
    "        \n",
    "        return (batch_size, n_input * n_feature)\n",
    "        \"\"\"\n",
    "        self.inout_X_shape = X.shape\n",
    "        output = X.reshape([self.inout_X_shape[0], self.inout_X_shape[1] * self.inout_X_shape[2]])\n",
    "        return output\n",
    "    \n",
    "    def backward(self, X):\n",
    "        output = X.reshape(self.inout_X_shape)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 2, 2)"
      ]
     },
     "execution_count": 460,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = np.array([[[1,2],[2,3]],[[4,5],[6,7]],[[8,9],[10,11]]])\n",
    "A.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = Flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1,  2,  2,  3],\n",
       "       [ 4,  5,  6,  7],\n",
       "       [ 8,  9, 10, 11]])"
      ]
     },
     "execution_count": 462,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f.forward(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 542,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "default_dnn_design = {\n",
    "    'learning_rate':0.001,\n",
    "    'total_layer':3,\n",
    "    'func_layer1':'tanh',\n",
    "    'func_layer2':'tanh',\n",
    "    'func_layer3':'softmax',\n",
    "    'node_layer0':786, \n",
    "    'node_layer1':400,\n",
    "    'node_layer2':200,\n",
    "    'node_layer3':10,\n",
    "    'initializer':'SimpleInitializer',\n",
    "    'initializer_sigma':0.05,\n",
    "    'optimizer':'SGD',\n",
    "}\n",
    "\n",
    "class ScratchDeepNeuralNetrowkClassifier2():\n",
    "    \"\"\"\n",
    "    ディープニューラルネットワーク分類器\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_epoch, batch_size, verbose = False):\n",
    "        self.verbose = verbose\n",
    "        self.batch_size = batch_size\n",
    "        self.n_epoch = n_epoch\n",
    "        self.loss = 0\n",
    "        self.loss_val = 0\n",
    "        self.activation_func = 0\n",
    "        self.affine_func = 0\n",
    "        self.n_layer = 0\n",
    "        self.layer_instance = [0 for _ in range(64)]\n",
    "        #self.activation_func = [0 for _ in range(self.dnn_design.get('total_layer'))]\n",
    "        #self.affine_func = [0 for _ in range(self.dnn_design.get('total_layer'))]\n",
    "        #self.n_layer = self.dnn_design.get('total_layer')\n",
    "        \n",
    "        #各インスタンスを生成\n",
    "        #initializerインスタンス\n",
    "        \n",
    "    def _crossentropy(self, y_pred, y):\n",
    "        #クロスエントロピーを計算する\n",
    "        INF_AVOIDANCE = 1e-8\n",
    "        cross_entropy = -1 * y * np.log(y_pred + INF_AVOIDANCE)\n",
    "        return np.sum(cross_entropy, axis=1)\n",
    "    \n",
    "    def add_layer(self, model):\n",
    "        self.layer_instance[self.n_layer] = model\n",
    "        self.n_layer += 1\n",
    "        return\n",
    "    \n",
    "    def delet_all_layer(self):\n",
    "        #add_layerでセットしたlayer情報を全てクリアする\n",
    "        self.layer_instance[0:self.n_layer] = 0\n",
    "        self.n_layer = 0\n",
    "        \n",
    "        return\n",
    "    \n",
    "    def fit(self, X, y, X_val=None, y_val=None):\n",
    "        #lossの記録用の配列を用意\n",
    "        self.loss = [[0 for i in range(X.shape[0])] for j in range(self.n_epoch)]\n",
    "        self.loss_val = [[0 for i in range(X.shape[0])] for j in range(self.n_epoch)]\n",
    "        \n",
    "        i = 0\n",
    "        get_mini_batch = GetMiniBatch(x_train, y_train, self.batch_size)\n",
    "        for epoch in range(self.n_epoch):\n",
    "            loop_count = 0\n",
    "            sum_loss = 0\n",
    "            for mini_X_train, mini_y_train in get_mini_batch:\n",
    "                X = mini_X_train\n",
    "                #Forwardの計算\n",
    "                for layer in range(self.n_layer):\n",
    "                    #X = self.affine_func[layer].forward(X)\n",
    "                    #X = self.activation_func[layer].forward(X)\n",
    "                    X = self.layer_instance[layer].forward(X)\n",
    "                    #print(\"layer:{} X.shape:{}\".format(layer, X.shape))\n",
    "                \n",
    "                #Loss計算\n",
    "                #print(\"X.shape:{} Y.shape:{}\".format(X.shape, mini_y_train.shape))\n",
    "                sum_loss += self._crossentropy(X, mini_y_train)\n",
    "                    \n",
    "                #Backwardの計算\n",
    "                dz = mini_y_train\n",
    "                for layer in reversed(range(0, self.n_layer)):\n",
    "                    #dz = self.activation_func[layer].backward(dz)\n",
    "                    #dz = self.affine_func[layer].backward(dz)\n",
    "                    dz = self.layer_instance[layer].backward(dz)\n",
    "                    #print(\"layer:{} dz.shape:{}\".format(layer, dz.shape))\n",
    "                \n",
    "                loop_count += 1\n",
    "                \n",
    "            #Epoch毎のLoss計算結果表示\n",
    "            self.loss[i] = sum_loss / loop_count\n",
    "            if X_val is not None and y_val is not None:\n",
    "                y_val_pred = self._predict(X_val)\n",
    "                self.loss_val[i] = self._crossentropy(y_val_pred, y_val)\n",
    "                \n",
    "            if self.verbose:\n",
    "                #verboseをTrueにした際は学習過程などを出力する\n",
    "                print(\"Epoch:{} Loss:{} Loss(val):{}\".format(i, self.loss[i], self.loss_val[i]))\n",
    "                \n",
    "            i +=1\n",
    "            \n",
    "        return\n",
    "    \n",
    "    def predict(self, X):\n",
    "        #Forwardの計算\n",
    "        for layer in range(self.n_layer):\n",
    "            #X = self.affine_func[layer].forward(X)\n",
    "            #X = self.activation_func[layer].forward(X)\n",
    "            X = self.layer_instance[layer].forward(X)\n",
    "        \n",
    "        max_val = np.max(X, axis=1)\n",
    "        mask = np.ones_like(X)\n",
    "        X[X == max_val[:,np.newaxis]] = 1\n",
    "        X[X != mask] = 0        \n",
    "        \n",
    "        return X\n",
    "\n",
    "    def _predict(self, X):\n",
    "        #Forwardの計算\n",
    "        for layer in range(self.n_layer):\n",
    "            #X = self.affine_func[layer].forward(X)\n",
    "            #X = self.activation_func[layer].forward(X)  \n",
    "            X = self.layer_instance[layer].forward(X)\n",
    "        \n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 543,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class FC2:\n",
    "    \"\"\"\n",
    "    ノード数n_nodes1からn_nodes2への全結合層\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_nodes1 : int\n",
    "      前の層のノード数\n",
    "    n_nodes2 : int\n",
    "      後の層のノード数\n",
    "    initializer : 初期化方法のインスタンス\n",
    "    optimizer : 最適化手法のインスタンス\n",
    "    \"\"\"\n",
    "    def __init__(self, n_nodes1, n_nodes2, initializer, optimizer, dropout_rate=0.5):\n",
    "        self.optimizer = optimizer\n",
    "        # 初期化\n",
    "        # initializerのメソッドを使い、self.Wとself.Bを初期化する\n",
    "        self.W = initializer.W(n_nodes1, n_nodes2)\n",
    "        self.B = initializer.B(n_nodes2)\n",
    "        self.W_feedback = 0\n",
    "        self.B_feedback = 0\n",
    "        self.dZ = 0\n",
    "        self.dA = 0\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.mask = None\n",
    "        self.input_X_forward = 0\n",
    "        \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        フォワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (batch_size, n_nodes1)\n",
    "            入力\n",
    "        Returns\n",
    "        ----------\n",
    "        A : 次の形のndarray, shape (batch_size, n_nodes2)\n",
    "            出力\n",
    "        \"\"\" \n",
    "        self.input_X_forward = X\n",
    "        A = np.dot(X, self.W) + self.B\n",
    "        \n",
    "        return A\n",
    "    \n",
    "    def backward(self, dA):\n",
    "        \"\"\"\n",
    "        バックワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        dA : 次の形のndarray, shape (batch_size, n_nodes2)\n",
    "            後ろから流れてきた勾配\n",
    "        Returns\n",
    "        ----------\n",
    "        dZ : 次の形のndarray, shape (batch_size, n_nodes1)\n",
    "            前に流す勾配\n",
    "        \"\"\"\n",
    "        \n",
    "        #print(\"input_X_forward.shape:\",self.input_X_forward.shape)\n",
    "        #print(\"dA.shape:\",dA.shape)\n",
    "        #print(\"self X_forward:\",self.input_X_forward.shape)\n",
    "        #print(\"dA:\",dA.shape)\n",
    "        dW = np.dot(self.input_X_forward.T, dA)\n",
    "        #print(\"dW:\",dW.shape)\n",
    "        dZ = np.dot(dA, self.W.T)\n",
    "        self.dA = dA\n",
    "        self.dW = dW\n",
    "        self.dZ = dZ\n",
    "        \n",
    "        self.W_feedback = self.dW / self.dA.shape[0]\n",
    "        self.B_feedback = np.average(self.dA, axis=0)\n",
    "        \n",
    "        # 更新\n",
    "        #print(\"W.shape:\",self.W.shape)\n",
    "        self = self.optimizer.update(self)\n",
    "        return dZ\n",
    "    \n",
    "    def dropout_forward(self, X, flag):\n",
    "        if flag:\n",
    "            self.mask = np.random.rand(*X.shape) > self.dropout_rate\n",
    "            return X * self.mask\n",
    "        else:\n",
    "            return X * (1.0 - self.dropout_rate)\n",
    "        \n",
    "    def dropout_backward(self, X): \n",
    "        return X * self.mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 544,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Conv1d():\n",
    "    \"\"\"\n",
    "    ノード数n_nodes1からn_nodes2への全結合層\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_nodes1 : int\n",
    "      前の層のノード数\n",
    "    n_nodes2 : int\n",
    "      後の層のノード数\n",
    "    initializer : 初期化方法のインスタンス\n",
    "    optimizer : 最適化手法のインスタンス\n",
    "    \"\"\"\n",
    "    def __init__(self, n_input_hight, f_w, f_b, initializer, optimizer):\n",
    "        self.optimizer = optimizer\n",
    "        # 初期化\n",
    "        # initializerのメソッドを使い、self.Wとself.Bを初期化する\n",
    "        self.n_input_hight = n_input_hight\n",
    "        #self.n_input_width = n_input_width\n",
    "        self.W = f_w    #(n_output, n_ch, f_size)\n",
    "        self.B = f_b    #(1, n_ch, n_output)\n",
    "        self.n_output = self.W.shape[0]\n",
    "        self.n_input_ch = self.W.shape[1]\n",
    "        self.f_hight = f_w.shape[2]\n",
    "        self.n_output_hight = self.n_input_hight - self.f_hight + 1\n",
    "        self.input_X_forward = 0\n",
    "        self.output_X_forward = np.zeros((self.W.shape[0], self.n_output_hight))\n",
    "        self.W_feedback = np.zeros_like(self.W)\n",
    "        self.B_feedback = np.zeros_like(self.B)\n",
    "        self.Z_feedback = 0\n",
    "    \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        フォワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (batch_size, n_ch, n_feature1)\n",
    "            入力\n",
    "        Returns\n",
    "        ----------\n",
    "        A : 次の形のndarray, shape (batch_size, n_output, n_feature2)\n",
    "            出力\n",
    "        \"\"\" \n",
    "        \n",
    "        self.input_X_forward = X\n",
    "        batch_size = self.input_X_forward.shape[0]\n",
    "        A = np.zeros((batch_size, self.n_output, self.n_input_ch, self.n_output_hight))\n",
    "        B = self.B[0]\n",
    "        B = B.T\n",
    "        B = B[np.newaxis]\n",
    "        #batch方向の並列計算のためaxisを追加 (Batch, ch, hight) = > (batch, 1, ch, hight)\n",
    "        X = X[:,np.newaxis]\n",
    "        for h in range(self.n_output_hight):\n",
    "            h1 = h\n",
    "            h2 = h + self.f_hight\n",
    "            X_seg = X[:,:,:,h1:h2]\n",
    "            \n",
    "            #print(\"X:{} W:{}\\n\".format(X_seg.shape, self.W.shape))\n",
    "            #アダマール積 (batch, 1, ch, filter_size) * (n_output, ch, filter_size)\n",
    "            tmp = np.sum(X_seg * self.W, axis=3)\n",
    "            #print(\"tmp1.shape:\",tmp.shape)\n",
    "            #print(\"B.shape:\",B.shape)\n",
    "            tmp = tmp + B\n",
    "            A[:,:,:,h] = tmp\n",
    "\n",
    "        A = np.sum(A, axis=2)\n",
    "        return A\n",
    "    \n",
    "    def backward(self, dA):\n",
    "        \"\"\"\n",
    "        バックワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        dA : 次の形のndarray, shape (batch_size, n_output, n_feature2)\n",
    "            後ろから流れてきた勾配\n",
    "        Returns\n",
    "        ----------\n",
    "        dZ : 次の形のndarray, shape (batch_size, n_ch, n_feature1)\n",
    "            前に流す勾配\n",
    "        \"\"\"\n",
    "        \n",
    "        batch_size = self.input_X_forward.shape[0]\n",
    "        \n",
    "        #Wについて\n",
    "        #X.shape (batch_size, n_input * n_output, n_feature1)\n",
    "        X = np.tile(self.input_X_forward, (dA.shape[1] ,1))\n",
    "        #dL.shape (n_input * n_output, n_featue2 )\n",
    "        dL = np.zeros((dA.shape[0], X.shape[1], dA.shape[2]))\n",
    "        for i in range(self.n_output):\n",
    "            o1 = i\n",
    "            o2 = i + self.n_input_ch\n",
    "            tmp = dA[:,i][:,np.newaxis,:]\n",
    "            dL[:,o1:o2] = np.tile(tmp, (self.n_input_ch ,1))\n",
    "        \n",
    "        #入力の特徴量数 - 出力の特徴量数 +1\n",
    "        loop = self.n_input_hight - self.n_output_hight + 1\n",
    "        dW_tmp = np.zeros((batch_size, self.n_output, loop))\n",
    "        for i in range(loop):\n",
    "            i1 = i\n",
    "            i2 = i + self.n_output_hight\n",
    "            dX_seg = X[:,:, i1:i2]\n",
    "            dW_tmp[:,:,i] = np.sum(dL * dX_seg, axis=2)\n",
    "        \n",
    "        #bacth方向の平均をとる\n",
    "        dW_tmp2 = np.average(dW_tmp, axis=0)     \n",
    "        #計算結果をフィルタサイズに整形\n",
    "        for i in range(dW_tmp2.shape[0]):\n",
    "            o1 = i\n",
    "            o2 = i + self.n_input_ch\n",
    "            self.W_feedback[i] = dW_tmp2[o1:o2]\n",
    "\n",
    "        #Bについて\n",
    "        #(batch_size, n_output, n_feature2)\n",
    "        dB = np.sum(dA, axis=2)\n",
    "        dB = np.average(dB, axis=0) #bacth方向の平均をとる\n",
    "        for i in range(self.B.shape[1]):\n",
    "            self.B_feedback[:,i] = dB\n",
    "        \n",
    "        #Zについて Output数回す\n",
    "        self.Z_feedback = np.zeros_like(self.input_X_forward)\n",
    "        for i in range(self.n_output):\n",
    "            #損失(行列)の端の処理のため、列の前後に0列を追加（フィルタサイズから計算）\n",
    "            dA_padding = np.zeros([batch_size, 1, self.f_hight-1])\n",
    "            dA_tmp = dA[:,i][:,np.newaxis,:]\n",
    "            #print(\"dA_tmp.shape1:\",dA_tmp.shape)\n",
    "            dA_tmp = np.concatenate((dA_tmp, dA_padding), axis=2)\n",
    "            dA_tmp = np.concatenate((dA_padding, dA_tmp), axis=2)\n",
    "            #print(\"dA_tmp.shape2:\",dA_tmp.shape)\n",
    "            dA_tmp = np.tile(dA_tmp, (self.n_input_ch ,1))\n",
    "            dZ_seg = np.zeros_like(self.Z_feedback)\n",
    "            \n",
    "            for h in range(self.n_input_hight):\n",
    "                h1 = h\n",
    "                h2 = h + self.f_hight\n",
    "                dA_seg = dA_tmp[:,:,h1:h2]\n",
    "                #並列計算工夫\n",
    "                dA_seg = np.fliplr(dA_seg.T).T\n",
    "                dZ_seg[:,:,h] = np.sum(dA_seg * self.W[i], axis=2)\n",
    "                \n",
    "            self.Z_feedback += dZ_seg #出力数分足し算\n",
    "\n",
    "        # 更新\n",
    "        self = self.optimizer.update(self)\n",
    "        return self.Z_feedback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 545,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#CNN Filter(重み、バイアス)\n",
    "f_w = np.ones((1,1,28))\n",
    "f_b = np.array([[[1]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 546,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "CNN2 = Conv1d(x_train.shape[2], f_w, f_b, initializer, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 547,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "A = CNN2.forward(mini_X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 548,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 1, 757)\n"
     ]
    }
   ],
   "source": [
    "print(A.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 549,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[1., 2., 3., ..., 3., 2., 1.]],\n",
       "\n",
       "       [[1., 2., 3., ..., 3., 2., 1.]],\n",
       "\n",
       "       [[1., 2., 3., ..., 3., 2., 1.]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[1., 2., 3., ..., 3., 2., 1.]],\n",
       "\n",
       "       [[1., 2., 3., ..., 3., 2., 1.]],\n",
       "\n",
       "       [[1., 2., 3., ..., 3., 2., 1.]]])"
      ]
     },
     "execution_count": 549,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CNN2.backward(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 562,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "CNN2 = ScratchDeepNeuralNetrowkClassifier2(5, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 563,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#DNNデザイン\n",
    "CNN2.add_layer(Conv1d(x_train.shape[2], f_w, f_b, initializer, optimizer))\n",
    "CNN2.add_layer(Flatten())\n",
    "CNN2.add_layer(FC2(f_w.shape[0] * (x_train.shape[2] - f_w.shape[2] + 1), 100, initializer, optimizer))\n",
    "CNN2.add_layer(Sigmoid())\n",
    "CNN2.add_layer(FC2(100, 10, initializer, optimizer))\n",
    "CNN2.add_layer(softmax())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 564,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "CNN2.fit(x_train, y_train, x_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 565,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred = CNN2.predict(x_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 556,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pred=\n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "Yval=\n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Pred=\\n\", y_pred)\n",
    "print(\"Yval=\\n\", y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 557,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score=0.610\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "print(\"Accuracy score={:.3f}\".format(accuracy_score(y_pred, y_val)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 561,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYwAAAEWCAYAAAB1xKBvAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAGiJJREFUeJzt3XuUVeWd5vHvA1WKrRAvIMglgol3aLAtaE2EYOylSBNp\nr4ioo+noxBgv0TbSXqLjMiutzqjpiZEwNkHToNDe2lEjxmgsXeOFgiluXgghglVRq8AbaghQ/OaP\ns3FOSF1eqmrXqSqez1p7uevd797n93q0nnr3PmdvRQRmZmYt6VHqAszMrGtwYJiZWRIHhpmZJXFg\nmJlZEgeGmZklcWCYmVkSB4ZZK0gaKikklXXga46XVNNRr2e2PQeGmZklcWCYmVkSB4Z1C5IGSnpI\nUr2k30u6tGjbjZIelDRP0gZJiyWNLNp+qKTfSPpQ0gpJJxVt203S/5C0RtJHkl6UtFvRS0+TtFbS\nOknXNlHb30p6V1LPoraTJS3N1sdIqpL0saT3JN2eOObm6p4o6bVsvLWS/ilr7yvp8Wyf9yW9IMm/\nByyJ/0OxLi/7hfe/gSXAIOA44HJJJxR1mwz8B7A3MBd4VFK5pPJs36eBfYFLgDmSDs72++/AkcBX\nsn2/D2wtOu4xwMHZa/5A0qHb1xcRrwCfAl8vaj4rqwPgx8CPI6IP8CVgfsKYW6r734D/GhG9geHA\ns1n7lUAN0A/oD1wD+P5AlsSBYd3BaKBfRNwUEZsiYjXwv4Azi/osiogHI2IzcDvQCzgqW/YA/iXb\n91ngcWBqFkTfBC6LiNqIaIiI/xMRfyo67n+LiD9GxBIKgTWSxt0PTAWQ1BuYmLUBbAa+LKlvRHwS\nES8njLnJuouOeZikPhHxQUQsLmrfD9g/IjZHxAvhG8pZIgeGdQf7AwOz0ywfSvqQwl/O/Yv6vL1t\nJSK2Uvgre2C2vJ21bbOGwkylL4Vg+V0zr/1u0fpnFH6JN2YucIqkXYFTgMURsSbb9o/AQcAbkhZK\nmtTsaAuaqxvgVAqhtEbS85KOztpvA1YBT0taLWl6wmuZAQ4M6x7eBn4fEXsWLb0jYmJRnyHbVrKZ\nw2DgD9kyZLvz+F8EaoF1wEYKp4naJCJeo/AL/UT+/HQUEfHbiJhK4dTSLcCDknZv4ZDN1U1ELIyI\nydkxHyU7zRURGyLiyog4ADgJuELScW0dn+0cHBjWHbwKbJB0dXaRuqek4ZJGF/U5UtIp2fcmLgf+\nBLwMvEJhZvD97JrGeOAbwAPZX++zgNuzi+o9JR2dzRJaYy5wGTCOwvUUACSdLalf9nofZs1bG9m/\nWJN1S9pF0jRJX8hOwX287XiSJkn6siQBHwENCa9lBjgwrBuIiAZgEjAK+D2FmcE9wBeKuv0nMAX4\nADgHOCU7h7+Jwi/aE7P9fgqcGxFvZPv9E7AMWAi8T2EG0Nr/b+4HvgY8GxHritonACskfULhAviZ\nEfHHFsbcUt3nAG9J+hj4NjAtaz8QeAb4BHgJ+GlEPNfK8dhORr7eZd2dpBuBL0fE2aWuxawr8wzD\nzMySODDMzCyJT0mZmVkSzzDMzCxJh92auSP07ds3hg4dWuoyzMy6jEWLFq2LiH4pfbtVYAwdOpSq\nqqpSl2Fm1mVIWtNyrwKfkjIzsyQODDMzS+LAMDOzJN3qGoaZ7Xw2b95MTU0NGzduLHUpnVqvXr0Y\nPHgw5eXlrT6GA8PMurSamhp69+7N0KFDKdxT0bYXEaxfv56amhqGDRvW6uP4lJSZdWkbN25kn332\ncVg0QxL77LNPm2dhDgwz6/IcFi1rj39HDgwzM0viwDAza6M99mjqybzdiwPDzMyS5BYYkmZJqpO0\nvJk+4yVVS1oh6fmi9rckLcu2+V4fZtYlRARXXXUVw4cPZ8SIEcybNw+Ad955h3HjxjFq1CiGDx/O\nCy+8QENDA+edd97nfe+4444SV9+yPD9WOxv4CXBfYxsl7UnhsZITImKtpH2363Lsdo+xNDNr3uWX\nQ3V1+x5z1Ci4886krg8//DDV1dUsWbKEdevWMXr0aMaNG8fcuXM54YQTuPbaa2loaOCzzz6jurqa\n2tpali8v/E394YcftnD00stthhERlRSegdyUs4CHI2Jt1r8ur1rMzDrCiy++yNSpU+nZsyf9+/fn\na1/7GgsXLmT06NH8/Oc/58Ybb2TZsmX07t2bAw44gNWrV3PJJZfw1FNP0adPn1KX36JSfnHvIKBc\n0m+A3sCPI2LbbCSAZyQ1AD+LiJklqtHMupLEmUBHGzduHJWVlTzxxBOcd955XHHFFZx77rksWbKE\nBQsWMGPGDObPn8+sWbNKXWqzSnnRuww4Evh74ATgekkHZduOiYhRwInAxZLGNXUQSRdKqpJUVV9f\nn3vRZmZNGTt2LPPmzaOhoYH6+noqKysZM2YMa9asoX///lxwwQV861vfYvHixaxbt46tW7dy6qmn\ncvPNN7N48eJSl9+iUs4waoD1EfEp8KmkSmAksDIiaqFwmkrSI8AYoLKxg2Szj5kAFRUVft6smZXM\nySefzEsvvcTIkSORxK233sqAAQO49957ue222ygvL2ePPfbgvvvuo7a2lvPPP5+tW7cC8KMf/ajE\n1bcs12d6SxoKPB4RwxvZdiiFi+InALsArwJnAr8HekTEBkm7A78CboqIp1p6vYqKivADlMx2Lq+/\n/jqHHnpoqcvoEhr7dyVpUURUpOyf2wxD0v3AeKCvpBrgBqAcICJmRMTrkp4ClgJbgXsiYrmkA4BH\nsq+xlwFzU8LCzMzylVtgRMTUhD63Abdt17aawqkpMzPrRPxNbzMzS+LAMDOzJA4MMzNL4sAwM7Mk\nDgwzM0viwDAz60DNPTvjrbfeYvjwv/jaWqfhwDAzsySlvDWImVm7KsXdzadPn86QIUO4+OKLAbjx\nxhspKyvjueee44MPPmDz5s3cfPPNTJ48eYded+PGjVx00UVUVVVRVlbG7bffzrHHHsuKFSs4//zz\n2bRpE1u3buWhhx5i4MCBnHHGGdTU1NDQ0MD111/PlClT2jLsRjkwzMzaYMqUKVx++eWfB8b8+fNZ\nsGABl156KX369GHdunUcddRRnHTSSWR3sEhy1113IYlly5bxxhtvcPzxx7Ny5UpmzJjBZZddxrRp\n09i0aRMNDQ08+eSTDBw4kCeeeAKAjz76KJexOjDMrNsoxd3NjzjiCOrq6vjDH/5AfX09e+21FwMG\nDOB73/selZWV9OjRg9raWt577z0GDBiQfNwXX3yRSy65BIBDDjmE/fffn5UrV3L00Ufzwx/+kJqa\nGk455RQOPPBARowYwZVXXsnVV1/NpEmTGDt2bC5j9TUMM7M2Ov3003nwwQeZN28eU6ZMYc6cOdTX\n17No0SKqq6vp378/GzdubJfXOuuss3jsscfYbbfdmDhxIs8++ywHHXQQixcvZsSIEVx33XXcdNNN\n7fJa2/MMw8ysjaZMmcIFF1zAunXreP7555k/fz777rsv5eXlPPfcc6xZs2aHjzl27FjmzJnD17/+\ndVauXMnatWs5+OCDWb16NQcccACXXnopa9euZenSpRxyyCHsvffenH322ey5557cc889OYzSgWFm\n1maHH344GzZsYNCgQey3335MmzaNb3zjG4wYMYKKigoOOeSQHT7md77zHS666CJGjBhBWVkZs2fP\nZtddd2X+/Pn84he/oLy8nAEDBnDNNdewcOFCrrrqKnr06EF5eTl33313DqPM+XkYHc3PwzDb+fh5\nGOna+jwMX8MwM7MkPiVlZtbBli1bxjnnnPNnbbvuuiuvvPJKiSpK48Awsy4vInboOw6lNmLECKrb\n+xuGLWiPyw8+JWVmXVqvXr1Yv359u/xC7K4igvXr19OrV682HcczDDPr0gYPHkxNTQ319fWlLqVT\n69WrF4MHD27TMRwYZtallZeXM2zYsFKXsVPwKSkzM0viwDAzsyQODDMzS+LAMDOzJA4MMzNL4sAw\nM7MkDgwzM0viwDAzsyS5BYakWZLqJC1vps94SdWSVkh6vqh9gqQ3Ja2SND2vGs3MLF2eM4zZwISm\nNkraE/gpcFJEHA6cnrX3BO4CTgQOA6ZKOizHOs3MLEFugRERlcD7zXQ5C3g4ItZm/euy9jHAqohY\nHRGbgAeAyXnVaWZmaUp5DeMgYC9Jv5G0SNK5Wfsg4O2ifjVZW6MkXSipSlKVbz5mZpafUt58sAw4\nEjgO2A14SdLLO3qQiJgJzITCI1rbtUIzM/tcKQOjBlgfEZ8Cn0qqBEZm7UOK+g0GaktQn5mZFSnl\nKan/BI6RVCbpr4C/BV4HFgIHShomaRfgTOCxEtZpZmbkOMOQdD8wHugrqQa4ASgHiIgZEfG6pKeA\npcBW4J6IWJ7t+11gAdATmBURK/Kq08zM0qg7PdawoqIiqqqqSl2GmVmXIWlRRFSk9PU3vc3MLIkD\nw8zMkjgwzMwsiQPDzMySODDMzCyJA8PMzJI4MMzMLIkDw8zMkjgwzMwsiQPDzMySODDMzCyJA8PM\nzJI4MMzMLIkDw8zMkjgwzMwsiQPDzMySODDMzCyJA8PMzJI4MMzMLIkDw8zMkjgwzMwsiQPDzMyS\nODDMzCyJA8PMzJI4MMzMLIkDw8zMkjgwzMwsSW6BIWmWpDpJy5vYPl7SR5Kqs+UHRdvekrQsa6/K\nq0YzM0tXluOxZwM/Ae5rps8LETGpiW3HRsS6dq/KzMxaJbcZRkRUAu/ndXwzM+tYpb6G8RVJSyX9\nUtLhRe0BPCNpkaQLmzuApAslVUmqqq+vz7daM7OdWJ6npFqyGPhiRHwiaSLwKHBgtu2YiKiVtC/w\nK0lvZDOWvxARM4GZABUVFdERhZuZ7YxKNsOIiI8j4pNs/UmgXFLf7Ofa7J91wCPAmFLVaWZmBSUL\nDEkDJClbH5PVsl7S7pJ6Z+27A8cDjX7SyszMOk5up6Qk3Q+MB/pKqgFuAMoBImIGcBpwkaQtwB+B\nMyMiJPUHHsmypAyYGxFP5VWnmZmlyS0wImJqC9t/QuFjt9u3rwZG5lWXmZm1Tqk/JWVmZl2EA8PM\nzJI4MMzMLIkDw8zMkjgwzMwsiQPDzMySODDMzCyJA8PMzJI4MMzMLElSYEi6TFIfFfybpMWSjs+7\nODMz6zxSZxjfjIiPKdwIcC/gHOBfcqvKzMw6ndTAUPbPicAvImJFUZuZme0EUgNjkaSnKQTGguz2\n41vzK8vMzDqb1LvV/iMwClgdEZ9J2hs4P7+yzMyss0mdYRwNvBkRH0o6G7gO+Ci/sszMrLNJDYy7\ngc8kjQSuBH4H3JdbVWZm1umkBsaWiAhgMvCTiLgL6J1fWWZm1tmkXsPYIOmfKXycdqykHmSPWzUz\ns51D6gxjCvAnCt/HeBcYDNyWW1VmZtbpJAVGFhJzgC9ImgRsjAhfwzAz24mk3hrkDOBV4HTgDOAV\nSaflWZiZmXUuqdcwrgVGR0QdgKR+wDPAg3kVZmZmnUvqNYwe28Iis34H9jUzs24gdYbxlKQFwP3Z\nz1OAJ/MpyczMOqOkwIiIqySdCnw1a5oZEY/kV5aZmXU2qTMMIuIh4KEcazEzs06s2cCQtAGIxjYB\nERF9cqnKzMw6nWYvXEdE74jo08jSu6WwkDRLUp2k5U1sHy/pI0nV2fKDom0TJL0paZWk6a0bmpmZ\ntac8P+k0G5jQQp8XImJUttwEIKkncBdwInAYMFXSYTnWaWZmCXILjIioBN5vxa5jgFURsToiNgEP\nULjpoZmZlVCpv0vxFUlLJf1S0uFZ2yDg7aI+NVlboyRdKKlKUlV9fX2etZqZ7dRKGRiLgS9GxF8D\n/xN4tDUHiYiZEVERERX9+vVr1wLNzOz/K1lgRMTHEfFJtv4kUC6pL1ALDCnqOjhrMzOzEipZYEga\nIEnZ+pislvXAQuBAScMk7QKcCTxWqjrNzKwg+Yt7O0rS/cB4oK+kGuAGsocuRcQM4DTgIklbgD8C\nZ2ZP9dsi6bvAAqAnMCsiVuRVp5mZpVHhd3T3UFFREVVVVaUuw8ysy5C0KCIqUvqW+lNSZmbWRTgw\nzMwsiQPDzMySODDMzCyJA8PMzJI4MMzMLIkDw8zMkjgwzMwsiQPDzMySODDMzCyJA8PMzJI4MMzM\nLIkDw8zMkjgwzMwsiQPDzMySODDMzCyJA8PMzJI4MMzMLIkDw8zMkjgwzMwsiQPDzMySODDMzCyJ\nA8PMzJI4MMzMLIkDw8zMkjgwzMwsiQPDzMyS5BYYkmZJqpO0vIV+oyVtkXRaUdtbkpZJqpZUlVeN\nZmaWLs8ZxmxgQnMdJPUEbgGebmTzsRExKiIqcqjNzMx2UG6BERGVwPstdLsEeAioy6sOMzNrHyW7\nhiFpEHAycHcjmwN4RtIiSRe2cJwLJVVJqqqvr8+jVDMzo7QXve8Ero6IrY1sOyYiRgEnAhdLGtfU\nQSJiZkRURERFv3798qrVzGynV1bC164AHpAE0BeYKGlLRDwaEbUAEVEn6RFgDFBZulLNzKxkgRER\nw7atS5oNPB4Rj0raHegRERuy9eOBm0pUppmZZXILDEn3A+OBvpJqgBuAcoCImNHMrv2BR7KZRxkw\nNyKeyqtOMzNLk1tgRMTUHeh7XtH6amBkHjWZmVnr+ZveZmaWxIFhZmZJHBhmZpbEgWFmZkkcGGZm\nlsSBYWZmSRwYZmaWxIFhZmZJHBhmZpbEgWFmZkkcGGZmlsSBYWZmSRwYZmaWxIFhZmZJHBhmZpbE\ngWFmZkkcGGZmlsSBYWZmSRwYZmaWxIFhZmZJHBhmZpbEgWFmZkkcGGZmlsSBYWZmSRwYZmaWxIFh\nZmZJHBhmZpbEgWFmZklyCwxJsyTVSVreQr/RkrZIOq2obYKkNyWtkjQ9rxrNzCxdnjOM2cCE5jpI\n6gncAjy9XdtdwInAYcBUSYflV6aZmaXILTAiohJ4v4VulwAPAXVFbWOAVRGxOiI2AQ8Ak/Op0szM\nUpXsGoakQcDJwN3bbRoEvF30c03W1tRxLpRUJamqvr6+/Qs1MzOgtBe97wSujoitbTlIRMyMiIqI\nqOjXr187lWZmZtsrK+FrVwAPSALoC0yUtAWoBYYU9RuctZmZWQmVLDAiYti2dUmzgccj4lFJZcCB\nkoZRCIozgbNKU6WZmW2TW2BIuh8YD/SVVAPcAJQDRMSMpvaLiC2SvgssAHoCsyJiRV51mplZmtwC\nIyKm7kDf87b7+UngyfauyczMWs/f9DYzsyQODDMzS+LAMDOzJA4MMzNL4sAwM7MkDgwzM0viwDAz\nsyQODDMzS+LAMDOzJA4MMzNL4sAwM7MkDgwzM0viwDAzsyQODDMzS+LAMDOzJIqIUtfQbiTVA2tK\nXccO6gusK3URHcxj3jl4zF3D/hHRL6VjtwqMrkhSVURUlLqOjuQx7xw85u7Hp6TMzCyJA8PMzJI4\nMEpvZqkLKAGPeefgMXczvoZhZmZJPMMwM7MkDgwzM0viwOgAkvaW9CtJv83+uVcT/SZIelPSKknT\nG9l+paSQ1Df/qtumrWOWdJukNyQtlfSIpD07rvp0Ce+ZJP1rtn2ppL9J3bezau2YJQ2R9Jyk1ySt\nkHRZx1ffOm15n7PtPSX9X0mPd1zVOYgILzkvwK3A9Gx9OnBLI316Ar8DDgB2AZYAhxVtHwIsoPDF\nxL6lHlPeYwaOB8qy9Vsa27/US0vvWdZnIvBLQMBRwCup+3bGpY1j3g/4m2y9N7Cyu4+5aPsVwFzg\n8VKPpy2LZxgdYzJwb7Z+L/APjfQZA6yKiNURsQl4INtvmzuA7wNd5VMKbRpzRDwdEVuyfi8Dg3Ou\ntzVaes/Ifr4vCl4G9pS0X+K+nVGrxxwR70TEYoCI2AC8DgzqyOJbqS3vM5IGA38P3NORRefBgdEx\n+kfEO9n6u0D/RvoMAt4u+rkma0PSZKA2IpbkWmX7atOYt/NNCn+9dTYp9TfVJ3XsnU1bxvw5SUOB\nI4BX2r3C9tfWMd9J4Y+9rXkV2FHKSl1AdyHpGWBAI5uuLf4hIkJS8ixB0l8B11A4RdOp5DXm7V7j\nWmALMKc1+1vnI2kP4CHg8oj4uNT15EnSJKAuIhZJGl/qetrKgdFOIuLvmtom6b1tU/JsmlrXSLda\nCtcpthmctX0JGAYskbStfbGkMRHxbrsNoBVyHPO2Y5wHTAKOi+xEcCfTbP0t9ClP2LczasuYkVRO\nISzmRMTDOdbZntoy5lOBkyRNBHoBfST9e0ScnWO9+Sn1RZSdYQFu488vAN/aSJ8yYDWFcNh2Ye3w\nRvq9Rde46N2mMQMTgNeAfqUeSzNjbPE9o3Duuvhi6Ks78n53tqWNYxZwH3BnqcfRUWPers94uvhF\n75IXsDMswD7Ar4HfAs8Ae2ftA4Eni/pNpPDJkd8B1zZxrK4SGG0aM7CKwjnh6myZUeoxNTHOv6gf\n+Dbw7WxdwF3Z9mVAxY68351xae2YgWMofGhjadH7OrHU48n7fS46RpcPDN8axMzMkvhTUmZmlsSB\nYWZmSRwYZmaWxIFhZmZJHBhmZpbEgWHWCUga3+XvZGrdngPDzMySODDMdoCksyW9Kqla0s+y5xx8\nIumO7BkPv5bUL+s7StLLRc/02Ctr/7KkZyQtkbRY0peyw+8h6cHsOSBzlN0LxqyzcGCYJZJ0KDAF\n+GpEjAIagGnA7kBVRBwOPA/ckO1yH3B1RPw1hW//bmufA9wVESOBrwDb7up7BHA5cBiFZy98NfdB\nme0A33zQLN1xwJHAwuyP/90o3FRxKzAv6/PvwMOSvgDsGRHPZ+33Av8hqTcwKCIeAYiIjQDZ8V6N\niJrs52pgKPBi/sMyS+PAMEsn4N6I+Oc/a5Su365fa++386ei9Qb8/6d1Mj4lZZbu18BpkvaFz59b\nvj+F/49Oy/qcBbwYER8BH0gam7WfAzwfhSfN1Uj6h+wYu2bPPDHr9PwXjFmiiHhN0nXA05J6AJuB\ni4FPgTHZtjoK1zkA/gswIwuE1cD5Wfs5wM8k3ZQd4/QOHIZZq/lutWZtJOmTiNij1HWY5c2npMzM\nLIlnGGZmlsQzDDMzS+LAMDOzJA4MMzNL4sAwM7MkDgwzM0vy/wDEqKRWkXjqwAAAAABJRU5ErkJg\ngg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1384799b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "loss = np.array(CNN2.loss)\n",
    "loss_ave = np.average(loss, axis=1)\n",
    "\n",
    "loss_val = np.array(CNN2.loss_val)\n",
    "loss_val_ave = np.average(loss_val, axis=1)\n",
    "\n",
    "plt.title(\"epoch vs loss\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.plot(loss_ave, \"r\", label=\"loss\")\n",
    "plt.plot(loss_val_ave, \"b\", label=\"val_loss\")\n",
    "plt.legend()\n",
    "#plt.yscale(\"Log\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 【問題8】（アドバンス課題）任意のストライド数\n",
    "ストライドは1限定の実装をしてきましたが、任意のストライド数に対応できるようにしてください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 529,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 757) (10, 100)\n"
     ]
    }
   ],
   "source": [
    "A = np.zeros([10,757])\n",
    "B = np.zeros([10,100])\n",
    "print(A.shape, B.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 531,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(757, 100)"
      ]
     },
     "execution_count": 531,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C = np.dot(A.T, B)\n",
    "C.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
