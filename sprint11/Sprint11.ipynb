{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class GetMiniBatch:\n",
    "    \"\"\"\n",
    "    ミニバッチを取得するイテレータ\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : 次の形のndarray, shape (n_samples, n_features)\n",
    "      学習データ\n",
    "    y : 次の形のndarray, shape (n_samples, 1)\n",
    "      正解値\n",
    "    batch_size : int\n",
    "      バッチサイズ\n",
    "    seed : int\n",
    "      NumPyの乱数のシード\n",
    "    \"\"\"\n",
    "    def __init__(self, X, y, batch_size = 10, seed=0):\n",
    "        self.batch_size = batch_size\n",
    "        np.random.seed(seed)\n",
    "        shuffle_index = np.random.permutation(np.arange(X.shape[0]))\n",
    "        self.X = X[shuffle_index]\n",
    "        self.y = y[shuffle_index]\n",
    "        self._stop = np.ceil(X.shape[0]/self.batch_size).astype(np.int)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self._stop\n",
    "\n",
    "    def __getitem__(self,item):\n",
    "        p0 = item*self.batch_size\n",
    "        p1 = item*self.batch_size + self.batch_size\n",
    "        return self.X[p0:p1], self.y[p0:p1]        \n",
    "\n",
    "    def __iter__(self):\n",
    "        self._counter = 0\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        if self._counter >= self._stop:\n",
    "            raise StopIteration()\n",
    "        p0 = self._counter*self.batch_size\n",
    "        p1 = self._counter*self.batch_size + self.batch_size\n",
    "        self._counter += 1\n",
    "        return self.X[p0:p1], self.y[p0:p1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class FC:\n",
    "    \"\"\"\n",
    "    ノード数n_nodes1からn_nodes2への全結合層\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_nodes1 : int\n",
    "      前の層のノード数\n",
    "    n_nodes2 : int\n",
    "      後の層のノード数\n",
    "    initializer : 初期化方法のインスタンス\n",
    "    optimizer : 最適化手法のインスタンス\n",
    "    \"\"\"\n",
    "    def __init__(self, n_nodes1, n_nodes2, initializer, optimizer):\n",
    "        self.optimizer = optimizer\n",
    "        # 初期化\n",
    "        # initializerのメソッドを使い、self.Wとself.Bを初期化する\n",
    "        self.W = initializer.W(n_nodes1, n_nodes2)\n",
    "        self.B = initializer.B(n_nodes2)\n",
    "        self.dZ = 0\n",
    "        self.dA = 0\n",
    "        self.input_X_forward = 0\n",
    "        \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        フォワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (batch_size, n_nodes1)\n",
    "            入力\n",
    "        Returns\n",
    "        ----------\n",
    "        A : 次の形のndarray, shape (batch_size, n_nodes2)\n",
    "            出力\n",
    "        \"\"\" \n",
    "        self.input_X_forward = X\n",
    "        A = np.dot(X, self.W) + self.B\n",
    "        \n",
    "        return A\n",
    "    \n",
    "    def backward(self, dA):\n",
    "        \"\"\"\n",
    "        バックワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        dA : 次の形のndarray, shape (batch_size, n_nodes2)\n",
    "            後ろから流れてきた勾配\n",
    "        Returns\n",
    "        ----------\n",
    "        dZ : 次の形のndarray, shape (batch_size, n_nodes1)\n",
    "            前に流す勾配\n",
    "        \"\"\"\n",
    "        \n",
    "        dW = np.dot(self.input_X_forward.T, dA)\n",
    "        dZ = np.dot(dA, self.W.T)\n",
    "        self.dA = dA\n",
    "        self.dW = dW\n",
    "        self.dZ = dZ\n",
    "        # 更新\n",
    "        self = self.optimizer.update(self)\n",
    "        return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Sigmoid:\n",
    "    \"\"\"\n",
    "    活性化関数 : Sigmoid\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_nodes1 : int\n",
    "      前の層のノード数\n",
    "    n_nodes2 : int\n",
    "      後の層のノード数\n",
    "    initializer : 初期化方法のインスタンス\n",
    "    optimizer : 最適化手法のインスタンス\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        # 初期化\n",
    "        self.input_X_forward = 0\n",
    "    \n",
    "    def _func(self, X):\n",
    "        return 1 / (1 + np.exp(-1 * X))\n",
    "    \n",
    "    def _func_diff(self, X):\n",
    "        return (1 - self._func(X)) * self._func(X)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        フォワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (batch_size, n_nodes1)\n",
    "            入力\n",
    "        Returns\n",
    "        ----------\n",
    "        A : 次の形のndarray, shape (batch_size, n_nodes2)\n",
    "            出力\n",
    "        \"\"\" \n",
    "        self.input_X_forward = X\n",
    "        A = self._func(X)\n",
    "        return A\n",
    "    \n",
    "    def backward(self, dA):\n",
    "        \"\"\"\n",
    "        バックワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        dA : 次の形のndarray, shape (batch_size, n_nodes2)\n",
    "            後ろから流れてきた勾配\n",
    "        Returns\n",
    "        ----------\n",
    "        dZ : 次の形のndarray, shape (batch_size, n_nodes1)\n",
    "            前に流す勾配\n",
    "        \"\"\"\n",
    "        \n",
    "        grad = self._func_diff(self.input_X_forward)\n",
    "        dZ = grad * dA\n",
    "        return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Tanh:\n",
    "    \"\"\"\n",
    "    活性化関数 : Sigmoid\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_nodes1 : int\n",
    "      前の層のノード数\n",
    "    n_nodes2 : int\n",
    "      後の層のノード数\n",
    "    initializer : 初期化方法のインスタンス\n",
    "    optimizer : 最適化手法のインスタンス\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        # 初期化\n",
    "        self.input_X_forward = 0\n",
    "    \n",
    "    def _func(self, X):\n",
    "        return np.tanh(X)\n",
    "    \n",
    "    def _func_diff(self, X):\n",
    "        return 1 - (self._func(X))**2\n",
    "        \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        フォワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (batch_size, n_nodes1)\n",
    "            入力\n",
    "        Returns\n",
    "        ----------\n",
    "        A : 次の形のndarray, shape (batch_size, n_nodes2)\n",
    "            出力\n",
    "        \"\"\" \n",
    "        self.input_X_forward = X\n",
    "        A = self._func(X)\n",
    "        return A\n",
    "    \n",
    "    def backward(self, dA):\n",
    "        \"\"\"\n",
    "        バックワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        dA : 次の形のndarray, shape (batch_size, n_nodes2)\n",
    "            後ろから流れてきた勾配\n",
    "        Returns\n",
    "        ----------\n",
    "        dZ : 次の形のndarray, shape (batch_size, n_nodes1)\n",
    "            前に流す勾配\n",
    "        \"\"\"\n",
    "        \n",
    "        grad = self._func_diff(self.input_X_forward)\n",
    "        dZ = grad * dA\n",
    "        return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ReLU:\n",
    "    \"\"\"\n",
    "    活性化関数 : Sigmoid\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_nodes1 : int\n",
    "      前の層のノード数\n",
    "    n_nodes2 : int\n",
    "      後の層のノード数\n",
    "    initializer : 初期化方法のインスタンス\n",
    "    optimizer : 最適化手法のインスタンス\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        # 初期化\n",
    "        self.input_X_forward = 0\n",
    "    \n",
    "    def _func(self, X):\n",
    "        return np.maximum(0, X)\n",
    "    \n",
    "    def _func_diff(self, X):\n",
    "        return np.where( x > 0, 1, 0)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        フォワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (batch_size, n_nodes1)\n",
    "            入力\n",
    "        Returns\n",
    "        ----------\n",
    "        A : 次の形のndarray, shape (batch_size, n_nodes2)\n",
    "            出力\n",
    "        \"\"\" \n",
    "        self.input_X_forward = X\n",
    "        A = self._func(X)\n",
    "        return A\n",
    "    \n",
    "    def backward(self, dA):\n",
    "        \"\"\"\n",
    "        バックワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        dA : 次の形のndarray, shape (batch_size, n_nodes2)\n",
    "            後ろから流れてきた勾配\n",
    "        Returns\n",
    "        ----------\n",
    "        dZ : 次の形のndarray, shape (batch_size, n_nodes1)\n",
    "            前に流す勾配\n",
    "        \"\"\"\n",
    "        \n",
    "        grad = self._func_diff(self.input_X_forward)\n",
    "        dZ = grad * dA\n",
    "        return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class softmax:\n",
    "    \"\"\"\n",
    "    活性化関数 : Sigmoid\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_nodes1 : int\n",
    "      前の層のノード数\n",
    "    n_nodes2 : int\n",
    "      後の層のノード数\n",
    "    initializer : 初期化方法のインスタンス\n",
    "    optimizer : 最適化手法のインスタンス\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        # 初期化\n",
    "        self.input_X_forward = 0\n",
    "        self.pred = 0\n",
    "    \n",
    "    def _func(self, X):\n",
    "        X = X - np.max(X)\n",
    "        tmp = np.exp(X)\n",
    "        denominator = np.sum(tmp, axis=1)\n",
    "        output = tmp / denominator[:, np.newaxis]\n",
    "        return output\n",
    "    \n",
    "    def _func_diff(self, X):\n",
    "        return X\n",
    "        \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        フォワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (batch_size, n_nodes1)\n",
    "            入力\n",
    "        Returns\n",
    "        ----------\n",
    "        A : 次の形のndarray, shape (batch_size, n_nodes2)\n",
    "            出力\n",
    "        \"\"\" \n",
    "        self.input_X_forward = X\n",
    "        A = self._func(X)\n",
    "        self.pred = A\n",
    "        return A\n",
    "    \n",
    "    def backward(self, dA):\n",
    "        \"\"\"\n",
    "        バックワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        dA : 次の形のndarray, shape (batch_size, n_nodes2)\n",
    "            後ろから流れてきた勾配\n",
    "        Returns\n",
    "        ----------\n",
    "        dZ : 次の形のndarray, shape (batch_size, n_nodes1)\n",
    "            前に流す勾配\n",
    "        \"\"\"\n",
    "        dZ = self.pred - dA\n",
    "        \n",
    "        return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class SimpleInitializer:\n",
    "    \"\"\"\n",
    "    ガウス分布によるシンプルな初期化\n",
    "    Parameters\n",
    "    ----------\n",
    "    sigma : float\n",
    "      ガウス分布の標準偏差\n",
    "    \"\"\"\n",
    "    def __init__(self, sigma = 0.01):\n",
    "        self.sigma = sigma\n",
    "        \n",
    "    def W(self, n_nodes1, n_nodes2):\n",
    "        W = self.sigma * np.random.randn(n_nodes1, n_nodes2)\n",
    "        return W\n",
    "    \n",
    "    def B(self, n_nodes2):\n",
    "        B = self.sigma * np.random.randn(1, n_nodes2)\n",
    "        return B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class XavierInitializer:\n",
    "    \"\"\"\n",
    "    Xavierによる初期化\n",
    "    Parameters\n",
    "    ----------\n",
    "    sigma : float\n",
    "      ガウス分布の標準偏差\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.n_prev_nodes = 1\n",
    "        pass\n",
    "        \n",
    "    def W(self, n_nodes1, n_nodes2):\n",
    "        self.n_prev_nodes = n_nodes1\n",
    "        W = np.random.randn(n_nodes1, n_nodes2) / np.sqrt(n_nodes1)\n",
    "        return W\n",
    "    \n",
    "    def B(self, n_nodes2):\n",
    "        B = np.random.randn(1, n_nodes2) / np.sqrt(self.n_prev_nodes)\n",
    "        return B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class HeInitializer:\n",
    "    \"\"\"\n",
    "    Heによる初期化\n",
    "    Parameters\n",
    "    ----------\n",
    "    sigma : float\n",
    "      ガウス分布の標準偏差\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.n_prev_nodes = 1\n",
    "        pass\n",
    "        \n",
    "    def W(self, n_nodes1, n_nodes2):\n",
    "        self.n_prev_nodes = n_nodes1\n",
    "        W = np.random.randn(n_nodes1, n_nodes2) * np.sqrt(2 / n_nodes1)\n",
    "        return W\n",
    "    \n",
    "    def B(self, n_nodes2):\n",
    "        B = np.random.randn(1, n_nodes2) * np.sqrt(2 / self.n_prev_nodes)\n",
    "        return B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SGD:\n",
    "    \"\"\"\n",
    "    確率的勾配降下法\n",
    "    Parameters\n",
    "    ----------\n",
    "    lr : 学習率\n",
    "    \"\"\"\n",
    "    def __init__(self, lr):\n",
    "        self.lr = lr\n",
    "    def update(self, layer):\n",
    "        \"\"\"\n",
    "        ある層の重みやバイアスの更新\n",
    "        Parameters\n",
    "        ----------\n",
    "        layer : 更新前の層のインスタンス\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        layer : 更新後の層のインスタンス\n",
    "        \"\"\"\n",
    "        layer.B = layer.B - self.lr * np.average(layer.dA, axis=0)\n",
    "        layer.W = layer.W - self.lr * layer.dW / layer.dA.shape[0]\n",
    "        \n",
    "        return layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class AdaGrad:\n",
    "    \"\"\"\n",
    "    確率的勾配降下法\n",
    "    Parameters\n",
    "    ----------\n",
    "    lr : 学習率\n",
    "    \"\"\"\n",
    "    def __init__(self, lr):\n",
    "        self.lr = lr\n",
    "        self.H_B = 1\n",
    "        self.H_W = 1\n",
    "    def update(self, layer):\n",
    "        \"\"\"\n",
    "        ある層の重みやバイアスの更新\n",
    "        Parameters\n",
    "        ----------\n",
    "        layer : 更新前の層のインスタンス\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        layer : 更新後の層のインスタンス\n",
    "        \"\"\"\n",
    "        \n",
    "        #dA, dWを更新＆保存\n",
    "        self.H_B = self.H_B + np.average(layer.dA)**2\n",
    "        self.H_W = self.H_W + np.average(layer.dW)**2\n",
    "        \n",
    "        layer.B = layer.B - self.lr * np.average(layer.dA, axis=0) / np.sqrt(self.H_B)\n",
    "        layer.W = layer.W - self.lr * layer.dW / layer.dA.shape[0] / np.sqrt(self.H_W)\n",
    "        \n",
    "        return layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "default_dnn_design = {\n",
    "    'learning_rate':0.001,\n",
    "    'total_layer':3,\n",
    "    'func_layer1':'tanh',\n",
    "    'func_layer2':'tanh',\n",
    "    'func_layer3':'softmax',\n",
    "    'node_layer0':786, \n",
    "    'node_layer1':400,\n",
    "    'node_layer2':200,\n",
    "    'node_layer3':10,\n",
    "    'initializer':'SimpleInitializer',\n",
    "    'initializer_sigma':0.05,\n",
    "    'optimizer':'SGD',\n",
    "}\n",
    "\n",
    "class ScratchDeepNeuralNetrowkClassifier():\n",
    "    \"\"\"\n",
    "    ディープニューラルネットワーク分類器\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_epoch, batch_size, dnn_design=default_dnn_design, verbose = False):\n",
    "        self.verbose = verbose\n",
    "        self.batch_size = batch_size\n",
    "        self.n_epoch = n_epoch\n",
    "        self.dnn_design = dnn_design\n",
    "        self.loss = 0\n",
    "        self.activation_func = [0 for _ in range(self.dnn_design.get('total_layer'))]\n",
    "        self.affine_func = [0 for _ in range(self.dnn_design.get('total_layer'))]\n",
    "        self.n_layer = self.dnn_design.get('total_layer')\n",
    "        \n",
    "        #各インスタンスを生成\n",
    "        #initializerインスタンス\n",
    "        if self.dnn_design.get('initializer') is 'XavierInitializer':\n",
    "            self.initializer = XavierInitializer()\n",
    "        elif self.dnn_design.get('initializer') is 'HeInitializer':\n",
    "            self.initializer = HeInitializer()\n",
    "        elif self.dnn_design.get('initializer') is 'SimpleInitializer':\n",
    "            self.initializer = SimpleInitializer(self.dnn_design.get('initializer_sigma'))\n",
    "        else:\n",
    "            self.initializer = SimpleInitializer()\n",
    "            \n",
    "        #optimizerインスタンス\n",
    "        if self.dnn_design.get('optimizer') is 'SGD':\n",
    "            self.optimizer = SGD(self.dnn_design.get('learning_rate'))\n",
    "        elif self.dnn_design.get('optimizer') is 'AdaGrad':\n",
    "            self.optimizer = AdaGrad(self.dnn_design.get('learning_rate'))\n",
    "        else:\n",
    "            self.optimizer = SGD(self.dnn_design.get('learning_rate'))\n",
    "            \n",
    "        #layerインスタンス\n",
    "        print(\"DNN layer design\")\n",
    "        for i in range(self.n_layer):\n",
    "            #layer designから各層のnode情報を取り出しaffine結合の関数インスタンスを作る\n",
    "            key1 = \"node_layer{}\".format(i)\n",
    "            key2 = \"node_layer{}\".format(i+1)\n",
    "            n_nodes1 = self.dnn_design.get(key1)\n",
    "            n_nodes2 = self.dnn_design.get(key2)\n",
    "            self.affine_func[i] = FC(n_nodes1, n_nodes2, self.initializer, self.optimizer)\n",
    "            \n",
    "            #layer designから各層の活性化関数情報を取り出し活性化関数インスタンスを作る\n",
    "            key3 = \"func_layer{}\".format(i+1)            \n",
    "            if self.dnn_design.get(key3) is 'sigmoid':\n",
    "                self.activation_func[i] = Sigmoid()\n",
    "            elif self.dnn_design.get(key3) is 'tanh':\n",
    "                self.activation_func[i] = Tanh()\n",
    "            elif self.dnn_design.get(key3) is 'ReLU':\n",
    "                self.activation_func[i] = ReLU()\n",
    "            elif self.dnn_design.get(key3) is 'softmax':\n",
    "                self.activation_func[i] = softmax()\n",
    "            else:\n",
    "                self.activation_func[i] = Sigmoid()\n",
    "                \n",
    "            print(\"layer{} : Func={} Node={}\".format(i+1, \n",
    "                                            self.dnn_design.get(key3),\n",
    "                                            self.dnn_design.get(key2)\n",
    "                                            ))\n",
    "        \n",
    "    def _crossentropy(self, y_pred, y):\n",
    "        #クロスエントロピーを計算する\n",
    "        cross_entropy = -1 * y * np.log(y_pred)\n",
    "        return np.sum(cross_entropy, axis=1)\n",
    "    \n",
    "    def fit(self, X, y, X_val=None, y_val=None):\n",
    "        #lossの記録用の配列を用意\n",
    "        self.loss = [[0 for i in range(X.shape[0])] for j in range(self.n_epoch)]\n",
    "        \n",
    "        i = 0\n",
    "        get_mini_batch = GetMiniBatch(x_train, y_train, self.batch_size)\n",
    "        for epoch in range(self.n_epoch):\n",
    "            loop_count = 0\n",
    "            sum_loss = 0\n",
    "            for mini_X_train, mini_y_train in get_mini_batch:\n",
    "                X = mini_X_train\n",
    "                #Forwardの計算\n",
    "                for layer in range(self.n_layer):\n",
    "                    X = self.affine_func[layer].forward(X)\n",
    "                    X = self.activation_func[layer].forward(X)\n",
    "                \n",
    "                #Loss計算\n",
    "                sum_loss += self._crossentropy(X, mini_y_train)\n",
    "                \n",
    "                #Backwardの計算\n",
    "                dz = mini_y_train\n",
    "                for layer in reversed(range(0, self.n_layer)):\n",
    "                    dz = self.activation_func[layer].backward(dz)\n",
    "                    dz = self.affine_func[layer].backward(dz)\n",
    "                \n",
    "                loop_count += 1\n",
    "                \n",
    "            #Epoch毎のLoss計算結果表示\n",
    "            self.loss[i] = sum_loss / loop_count\n",
    "            if self.verbose:\n",
    "                #verboseをTrueにした際は学習過程などを出力する\n",
    "                print(\"Epoch:{} Loss:{}\".format(i, self.loss[i]))\n",
    "            i +=1\n",
    "            \n",
    "        return\n",
    "    \n",
    "    def predict(self, X):\n",
    "        #Forwardの計算\n",
    "        for layer in range(self.n_layer):\n",
    "            X = self.affine_func[layer].forward(X)\n",
    "            X = self.activation_func[layer].forward(X)\n",
    "        \n",
    "        max_val = np.max(X, axis=1)\n",
    "        mask = np.ones_like(X)\n",
    "        X[X == max_val[:,np.newaxis]] = 1\n",
    "        X[X != mask] = 0        \n",
    "        \n",
    "        return X\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 試しに動かす"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(36000, 784)\n",
      "(24000, 784)\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import mnist\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "x_train = x_train.reshape(-1, 784)\n",
    "x_test = x_test.reshape(-1, 784)\n",
    "\n",
    "x_train = x_train.astype(np.float)\n",
    "x_test = x_test.astype(np.float)\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "\n",
    "enc = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "y_train_one_hot = enc.fit_transform(y_train[:, np.newaxis])\n",
    "y_test_one_hot = enc.transform(y_test[:, np.newaxis])\n",
    "\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train_one_hot, test_size=0.4)\n",
    "print(x_train.shape) # (48000, 784)\n",
    "print(x_val.shape) # (12000, 784)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#関数:tanh, ReLU, sigmoid, softmax\n",
    "dnn_layer_design = {\n",
    "    'learning_rate':0.005,\n",
    "    'total_layer':3,\n",
    "    'func_layer1':'tanh',\n",
    "    'func_layer2':'tanh',\n",
    "    'func_layer3':'softmax',\n",
    "    'node_layer0':784,\n",
    "    'node_layer1':400,\n",
    "    'node_layer2':200,\n",
    "    'node_layer3':10,\n",
    "    'initializer':'HeInitializer',\n",
    "    'initializer_sigma':0.01,\n",
    "    'optimizer':'AdaGrad',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DNN layer design\n",
      "layer1 : Func=tanh Node=400\n",
      "layer2 : Func=tanh Node=200\n",
      "layer3 : Func=softmax Node=10\n"
     ]
    }
   ],
   "source": [
    "dnn_clf = ScratchDeepNeuralNetrowkClassifier(3, 10, dnn_layer_design, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dnn_clf.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 10)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYwAAAEWCAYAAAB1xKBvAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1334e4160>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "loss = np.array(dnn_clf.loss)\n",
    "print(loss.shape)\n",
    "loss_ave = np.average(loss, axis=1)\n",
    "\n",
    "plt.title(\"epoch vs loss\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.plot(loss_ave, \"r\", label=\"loss\")\n",
    "#plt.plot(self.val_loss, \"b\", label=\"val_loss\")\n",
    "plt.legend()\n",
    "#plt.yscale(\"Log\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_pred = dnn_clf.predict(x_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score=0.927\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy score={:.3f}\".format(accuracy_score(y_pred, y_val)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
