{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 【問題1】機械翻訳の実行とコードリーディング\n",
    "Keras公式のサンプルコードで、短い英語からフランス語への変換が行えるのでこれを動かしてください。\n",
    "その上でこのサンプルコードの各部分がどういった役割かを読み取り、まとめてください。以下のようにどこからどこの行が何をしているかを記述してください。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "エンコーダー部を定義。encoderの出力は実際は使わず、encoder_statesのみをdecoderへ渡す。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "encoder_inputs = Input(shape=(None, num_encoder_tokens))\n",
    "encoder = LSTM(latent_dim, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
    "encoder_states = [state_h, state_c]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "デコーダー部分を定義。出力層のsoftmaxへ入力するように全結合をする。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "decoder_inputs = Input(shape=(None, num_decoder_tokens))\n",
    "# We set up our decoder to return full output sequences,\n",
    "# and to return internal states as well. We don't use the\n",
    "# return states in the training model, but we will use them in inference.\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_inputs,\n",
    "                                     initial_state=encoder_states)\n",
    "decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定義したエンコーダ、デコーダを渡して学習モデルを定義。更新式はAdagardを改良したRMSPROPを用いている。Lossは多クラス分類なのでcategorical_crossentropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "# Run training\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy')\n",
    "model.fit([encoder_input_data, decoder_input_data], decoder_target_data,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "推論に使うモデルを定義。学習モデルではデコーダ部分のLSTMへの入力はtargetデータであったが、推論モデルでは、1シーケンス前のsoftmaxで推測された結果が次のLSTMヘ入力される。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "\n",
    "decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "decoder_outputs, state_h, state_c = decoder_lstm(\n",
    "    decoder_inputs, initial_state=decoder_states_inputs)\n",
    "decoder_states = [state_h, state_c]\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs] + decoder_states_inputs,\n",
    "    [decoder_outputs] + decoder_states)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上で定義したモデルを使ってデコードを行う。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 学習結果"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"seq2seq/Learning.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"seq2seq/prediction.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 【問題2】イメージキャプショニングの学習済みモデルの実行\n",
    "上記実装において5. Test the modelの項目を実行してください。また、自身で用意した画像に対しても文章を生成してください。これらに対してどういった文章が出力されたかを記録して提出してください。\n",
    "データセットからの学習は行わず、学習済みの重みをダウンロードして利用します。\n",
    "注意点として、デフォルトで設定されている重みのファイル名と、ダウンロードできる重みのファイル名は異なっています。ここは書き換える必要があります。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"image-caption/sakana_kun.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"image-caption/caption.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "正しい表現：男、帽子  \n",
    "間違った表現：ネクタイ、テーブルの上"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 【問題3】Kerasで動かしたい場合はどうするかを調査\n",
    "PyTorchによる実装を動かしましたが、何らかの理由からKerasで動かしたい状況が考えられます。どういった手順を踏むことになるか調査し、できるだけ詳しく説明してください。\n",
    "特に今回はPyTorchのための学習済みの重みをKerasで使えるようにしたいので、その点については必ず触れてください。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.PytorchからKerasへコードの変更  \n",
    "2.mmdmmなどのツールを使いPytorchの学習モデルをKerasへ変更する。（https://github.com/Microsoft/MMdnn）  \n",
    "以下の図のように１度PytorchモデルをIRという中間ファイルへ変換し、それを目的のモデルへ変換（今回はkerasへ）する。\n",
    "\n",
    "<img src=\"image-caption/mmdnn_fig.png\">\n",
    "\n",
    "\n",
    "3.書き換えたモデルで再度学習"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 【問題4】（アドバンス課題）コードリーディングと書き換え\n",
    "モデル部分はmodel.pyに書かれていますが、Kerasではこのモデルがどのように記述できるかを考え、コーディングしてください。その際機械翻訳のサンプルコードが参考になります。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Original Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class EncoderCNN(nn.Module):\n",
    "    def __init__(self, embed_size):\n",
    "        \"\"\"Load the pretrained ResNet-152 and replace top fc layer.\"\"\"\n",
    "        super(EncoderCNN, self).__init__()\n",
    "        # ImageNet\n",
    "        resnet = models.resnet152(pretrained=True)  \n",
    "        # delete the last fc layer.\n",
    "        modules = list(resnet.children())[:-1]\n",
    "        self.resnet = nn.Sequential(*modules)\n",
    "        #全結合でVectorizeされた文字データと同じ次元に調整\n",
    "        self.linear = nn.Linear(resnet.fc.in_features, embed_size)\n",
    "        self.bn = nn.BatchNorm1d(embed_size, momentum=0.01)\n",
    "        \n",
    "    def forward(self, images):\n",
    "        \"\"\"Extract feature vectors from input images.\"\"\"\n",
    "        # 層の凍結 imagenetは学習しない\n",
    "        with torch.no_grad():\n",
    "            features = self.resnet(images)\n",
    "        #Flatten\n",
    "        features = features.reshape(features.size(0), -1)\n",
    "        features = self.bn(self.linear(features))\n",
    "        return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, embed_size, hidden_size, vocab_size, num_layers, max_seq_length=20):\n",
    "        \"\"\"Set the hyper-parameters and build the layers.\"\"\"\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        #文字のVectorize\n",
    "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
    "        #embed_size=(seq, batch, feature)　Hidden_size:(node,node) num_layer:(Vocabの数 LSTMの繰り返し回数)\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, batch_first=True)\n",
    "        #LSTMの出力をSoftmaxに入れるためにshape調整\n",
    "        self.linear = nn.Linear(hidden_size, vocab_size)\n",
    "        self.max_seg_length = max_seq_length\n",
    "        \n",
    "    #training\n",
    "    def forward(self, features, captions, lengths):\n",
    "        \"\"\"Decode image feature vectors and generates captions.\"\"\"\n",
    "        #正解データ(caption)をベクトル変換\n",
    "        embeddings = self.embed(captions)\n",
    "        #エンコーダで抽出した画像の特徴量とベクトル化したcaptionをconcat\n",
    "        embeddings = torch.cat((features.unsqueeze(1), embeddings), 1)\n",
    "        packed = pack_padded_sequence(embeddings, lengths, batch_first=True) \n",
    "        #LSTMへ入力\n",
    "        hiddens, _ = self.lstm(packed)\n",
    "        #softmaxへの入力\n",
    "        outputs = self.linear(hiddens[0])\n",
    "        return outputs\n",
    "    \n",
    "    #Prediction\n",
    "    def sample(self, features, states=None):\n",
    "        \"\"\"Generate captions for given image features using greedy search.\"\"\"\n",
    "        sampled_ids = []\n",
    "        inputs = features.unsqueeze(1)\n",
    "        for i in range(self.max_seg_length):\n",
    "            hiddens, states = self.lstm(inputs, states)          # hiddens: (batch_size, 1, hidden_size)\n",
    "            outputs = self.linear(hiddens.squeeze(1))            # outputs:  (batch_size, vocab_size)\n",
    "            _, predicted = outputs.max(1)                        # predicted: (batch_size)\n",
    "            sampled_ids.append(predicted)\n",
    "            inputs = self.embed(predicted)                       # inputs: (batch_size, embed_size)\n",
    "            inputs = inputs.unsqueeze(1)                         # inputs: (batch_size, 1, embed_size)\n",
    "        sampled_ids = torch.stack(sampled_ids, 1)                # sampled_ids: (batch_size, max_seq_length)\n",
    "        return sampled_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Dense, Reshape, Embedding, Concatenate, LSTM, Input, Activation\n",
    "from keras.applications.resnet50 import ResNet50\n",
    "import tensorflow as tf\n",
    "\n",
    "emb_size = 256\n",
    "num_of_words = 100\n",
    "hidden_size = 32\n",
    "input_size =(256,256,3)\n",
    "\n",
    "\"\"\"\n",
    "image_net = ResNet50(\n",
    "   weights='imagenet',\n",
    "   include_top=False,\n",
    "   input_shape=(224,224,3), trainable=False, pooling=\"ave\")\n",
    "\"\"\"\n",
    "#Encoder\n",
    "input_tensor = Input(shape=input_size)\n",
    "image_net = ResNet50(include_top=False, weights='imagenet', input_shape=input_size, pooling='avg')\n",
    "x = image_net(input_tensor)\n",
    "x = Dense(emb_size)(x)\n",
    "x = Reshape((1,emb_size))(x)\n",
    "\n",
    "#target caption\n",
    "target_caption = Input(shape=(None,))\n",
    "y = Embedding(num_of_words, emb_size)(target_caption)\n",
    "y = Concatenate(1)([x, y])\n",
    "#Decoder\n",
    "output = LSTM(hidden_size, return_sequences=True)(y)\n",
    "output = Dense(num_of_words)(output)\n",
    "output = Activation('softmax')(output)\n",
    "\n",
    "model = Model([input_tensor, target_caption], output)\n",
    "model.compile(optimizer=tf.train.AdamOptimizer(0.01),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 【問題5】（アドバンス課題）発展的調査\n",
    "他の言語の翻訳を行う場合は？\n",
    "問題1の実装を使い日本語と英語の翻訳を行いたい場合はどのような手順を踏むか考えてみましょう。\n",
    "機械翻訳の発展的手法にはどのようなものがある？  \n",
    "機械翻訳のための発展的手法にはどういったものがあるか調査してみましょう。  \n",
    "文章から画像生成するには？  \n",
    "イメージキャプショニングとは逆に文章から画像を生成する手法もあります。どういったものがあるか調査してみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
